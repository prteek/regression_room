---
title: "Analysis of price reductions"
author: "Prateek"
date: "2025-01-08"
categories: [logistic regression, probability]
toc: true
description: Modelling and analysing probability of house sale price reductions
---

## Introduction
In real estate universe, often listings on the market experience updates due to either changes to property specific information or due to an influence of markets. <br/>
The market influences prices of listings in major way and can lead to price increase or decrease post original advertisement.<br/>
Of interest to a real investment company are price revisions that are < 0, i.e. where prices of listings decrease. Such properties can offer immediate improvement to rental yield investment cases and shorter times to close due to comparatively lower demand.

The current analysis aspires to be able to understand where and based on what factors could price changes have happened using a historic listings dataset. This is to validate the analysis against subject matter knowledge and also expand the current understanding about price revision behaviours in the market. <br/>
A better understanding of factors associated with price reductions can help drive strategic decisions e.g. if prices are more likely to be reduced at a particular location in recent data then the market may be experiencing a contraction there.

The analysis also extends to prediction of probability of price reduction based on most informative inputs, to be able to have a mechanism where operations teams can prioritise negotiations and due diligence for such properties to benefit from the possibility of their weaker market demand.

## Data
The [data](https://raw.githubusercontent.com/prteek/regression_room/refs/heads/main/posts/price_revision/terminal_revisions.csv) is derived using 2 different data files [listings.csv](https://raw.githubusercontent.com/prteek/regression_room/refs/heads/main/posts/price_revision/listings.csv) and [revisions.csv](https://raw.githubusercontent.com/prteek/regression_room/refs/heads/main/posts/price_revision/revisions.csv). <br/>
These files individually contain basic attributes of properties and revised prices (both increased in decreased).

These are cleaned up for sensible choices of attributes in listing data and price revisions. There are no missing values and the data values are checked for the suitability of application. The script to cleanup and prepare data can be found [here](https://github.com/prteek/regression_room/blob/main/posts/price_revision/data_cleanup.R).

```{r}
#| echo: false
suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(knitr))
library(patchwork)
library(faraway)
library(this.path)
suppressPackageStartupMessages(library(pROC))
setwd(here())
options(width=200)
revisions <- read_csv('terminal_revisions.csv', show_col_types=F) %>%
                mutate(bedroom_count=as.factor(bedroom_count),
                location=relevel(as.factor(location), ref='Watford'),
                property_type=as.factor(property_type)
                )

kable(head(revisions))

```
The columns are self explanatory and the thing worth noting is that active days indicate the time elapsed from *listing_date to revision_date*.

We've converted *bedroom_count* to a qualitative type, since bedroom count has a highly non linear effect on prices and other attributes of property (1->2 != 2 -> 3). <br/>
Additionally, there are only a few different bedroom counts in the data and it could be analysed without treating this information as quantitative.

### EDA

There are unknown values in *tenure* which should likely be either of the other 2 categories. The proportion of these listings is about 8% in the data so we cannot drop these listings since their share is quite large. <br/>
Additionally, most often *flats* are observed to be leasehold (which is sensible given that land is not definitively owned) and *houses* are observed to be freehold. Thus suggesting a strong correlation between *tenure* and *property_type*, which means we can drop this column from analysis and use just the *property_type* for both pieces of information.

```{r}
#| echo: false
kable(revisions %>% group_by(tenure, property_type) %>% summarise(counts=n(), .groups='keep'))

```

We do not find a clear relationship between *asking_price* and properties that have had 5% reduction in prices. It is still possible for there to be some relationship but it is perhaps confounded by *location* or *property_type* etc. <br/>
We expect that inclusion of *location* and other factors in the model will clarify effect of *asking_price* on revisions. <br/>
Similar conclusion for *active_days*. Due to both of these variables having a long tail, these were log transformed and for the rest of the analysis we shall apply the same treatment.

```{r}
#| echo: false
p1 <- ggplot(revisions %>% filter(asking_price>0)) + geom_boxplot(aes(y=log(asking_price), x=is_5percent_reduced)) + theme_minimal()

p2 <- ggplot(revisions %>% filter(active_days>0)) + geom_boxplot(aes(y=log(active_days), x=is_5percent_reduced)) + theme_minimal()

p1|p2

```


There are locations which have had a higher share of properties reduced and this is encouraging as conditional on *location* effect other variables may be different. <br/>
We observe some association between reductions and *bedroom count* and *property type* too.

```{r}
#| echo: false
p1 <- ggplot(revisions %>%
        group_by(location) %>%
        mutate(prop_yes=sum(is_5percent_reduced == "yes") / n()) %>%
        ungroup(),
        aes(x = reorder(location, prop_yes), fill = is_5percent_reduced)
 ) +
  geom_bar(position = "fill", alpha=0.5) +  # "fill" shows proportions
  labs(title = "Location vs reductions", y = "Proportion", x="Location") +
  coord_flip() +
  theme_minimal()

p2 <- ggplot(revisions,
        aes(x = bedroom_count, fill = is_5percent_reduced)) +
  geom_bar(position = "fill", alpha=0.5) +  # "fill" shows proportions
  labs(title = "Bedrooms vs reductions", y = "Proportion", x="Bedrooms") +
  coord_flip() +
  theme_minimal()


p3 <- ggplot(revisions %>%
        group_by(property_type) %>%
        mutate(prop_yes=sum(is_5percent_reduced == "yes") / n()) %>%
        ungroup(),
        aes(x = reorder(property_type, prop_yes), fill = is_5percent_reduced)
 ) +
  geom_bar(position = "fill", alpha=0.5) +  # "fill" shows proportions
  labs(title = "Property type vs reductions", y = "Proportion", x="Property type") +
  coord_flip() +
  theme_minimal()

p1
p2 | p3
```


The proportion of properties undergoing price reductions is fairly (mean) stable over time, across almost all locations. We shall not focus on the time component in the rest of the analysis for simplification.

```{r}
#| echo: false
#| fig-width: 12
#| fig-height: 10
revisions_summary <- revisions %>%
  mutate(revision_month = floor_date(revision_date, unit = 'month')) %>%
  group_by(revision_month, location, is_5percent_reduced) %>%
  summarise(counts = n(), .groups = 'keep') %>%
  pivot_wider(names_from = is_5percent_reduced, values_from = counts, values_fill = 0) %>%
  mutate(proportion_yes = yes / (no + yes))

ggplot(revisions_summary) + geom_point(aes(x=revision_month, y=proportion_yes)) + geom_line(aes(x=revision_month, y=proportion_yes)) + facet_wrap(~location) + theme_minimal()


```


## Model

### Inference

```{r}
#| echo: false
train <- revisions[1:50000,] %>% mutate(is_5percent_reduced=if_else(is_5percent_reduced=='yes',1,0),
active_days=if_else(active_days==0, 0.001, active_days)) %>% filter(asking_price>0)
test <- revisions[50000:nrow(revisions),] %>% mutate(is_5percent_reduced=if_else(is_5percent_reduced=='yes',1,0),
active_days=if_else(active_days==0, 0.001, active_days)) %>% filter(asking_price>0)

lmod <- glm(is_5percent_reduced ~ property_type + bedroom_count + log(asking_price) + log(active_days) + log(active_days) + location, family=binomial, train)

beta <- coef(lmod)
formula(lmod)
cat(paste("Null deviance:", round(lmod$null.deviance,0)))
cat(paste("Model deviance:", round(lmod$deviance,0)))
cat(paste("Null df - Model df:", lmod$df.null-lmod$df.residual))
cat(paste("P-Value X^2:", 1-pchisq(lmod$null.deviance-lmod$deviance, lmod$df.null-lmod$df.residual)))

```

Even without a formal test we know that a chi-sq distribution with n degrees of freedom should have an expected value of n. In our case (14392-13776=616) is much larger than DoF: 39, so we can reject the hypothesis that model is not significantly different from the null model with no explanatory variables. <br/>
The chi-square p value suggests the same and we can continue analysing effects captured by the model. <br/>

### Effect of asking price and active days

Both of these variables have shapes characteristic of a *log* transform which was applied previously.
It can be observed that *asking_price* increasing in the lower end significantly affects the likelihood of price reduction. This could be due to the fact that on a lower baseline, a change of price can easily materialise into a 5% difference as opposed to at higher end of price where 5% reduction requires a very large £ value reduction. <br/>

```{r}
#| echo: false
par(mfrow=c(1,2))
n <- 30 # 3 bed house in Leicester
X <- model.matrix(lmod)[n,]
col_name = "log(asking_price)"
i_beta <- which(names(beta)==col_name)
X[col_name] = 0
contrib <- t(as.matrix(X)) %*% beta
plot(jitter(is_5percent_reduced, 0.1) ~ jitter(asking_price), data=train, pch='.', xlab='asking price', ylab='prob(is_5percent_reduced)')
curve(ilogit(contrib[1] + beta[i_beta]*log(x)), add=TRUE)
curve(ilogit(contrib[1] + beta[i_beta]*log(x) - beta['property_typehouse']*1), add=TRUE, lty=2)
legend("left", legend = c("house", "flat"), lty = c(1,2))
grid()

X <- model.matrix(lmod)[n,]
col_name = "log(active_days)"
i_beta <- which(names(beta)==col_name)
X[col_name] = 0
contrib <- t(as.matrix(X)) %*% beta
plot(jitter(is_5percent_reduced, 0.1) ~ jitter(active_days), data=train, pch='.', xlab='active days', ylab='prob(is_5percent_reduced)')
curve(ilogit(contrib[1] + beta[i_beta]*log(x)), add=TRUE)
curve(ilogit(contrib[1] + beta[i_beta]*log(x) - beta['property_typehouse']*1), add=TRUE, lty=2)
legend("right", legend = c("house", "flat"), lty = c(1,2))
grid()
par(mfrow=c(1,1))

# scale <- sqrt(diag(summary(lmod)$cov.unscaled))[2:length(beta)]
# beta_p <- beta[2:length(beta)]
# round(exp(cbind(odds_ratio=beta_p, lower=beta_p-1.96*scale, upper=beta_p+1.96*scale)),2)
```
*active_days* does not seem to strongly affect probability of price reduction across its range of values. Towards the initial range of values, the effect seems to be similar to *asking_price*. The small magnitude of change in probabilities makes the judgement of significance of this variable ambiguos from visual inspection of this single example. <br/>
We can test individual explanatory variables by fitting models that drop each variable once and compute the difference in deviance observed.

```{r}
drop1(lmod, test="Chi")

```
A formal evaluation suggests that all of the explanatory variables are significant. This clarifies confusion around *log(active_days)*. This is in contrast to what was observed during EDA, suggesting a confounding effect due to other variables in the model.

### Effect of location

The plot below gives a comparative assessment of which locations are associated with higher odds of price reduction. This is useful since the summary strips away effects of other explanatory variables and allows a *fair* comparison between locations (unlike the plot in EDA). <br/>
For example, *Hull* which appeared mid-table in EDA is now at the top. One prominent reason is that *Hull* has the lowest average *asking_price* (~200,000£) which as seen above, is associated with a very low probability of price reduction. This fact confounds the effect of location itself which is now revealed. <br/>
The overall result may be combined with other data sources to affirm market level behaviours which can be useful for internal know how and targeted acquisitions. <br/>
As an example in the chart, places with high odds of reduction:

* Sheffield
* Hull
* Mansfield
* Warrington

Are all clustered together in the north not very far from each other. Indicative of a broader market behaviour of North UK.

```{r}
#| echo: false
coefficients <- coef(lmod)

location_coeffs <- coefficients[grep("location", names(coefficients))]
odds <- exp(location_coeffs)

location_names <- gsub("location", "", names(location_coeffs))
location_odds <- data.frame(Location = location_names, odds = odds)

location_odds <- location_odds[order(-location_odds$odds),]

ggplot(location_odds, aes(x = reorder(Location, odds), y = odds)) +
  geom_bar(stat = "identity") +
  labs(title = "Odds of reduction (reference=Watford)",
       x = "Location", y = "Odds of reduction") +
  theme_minimal() +
  coord_flip()

```


### Diagnostics

#### Relationship between Predicted values and Residuals

We construct residual plot by gruping residuals into bins where bins are based on similar predictor values. The choice of number of bins is arbitrary and is made to ensure that we have roughly 500 observations per bin. <br/>

The deviance residuals are not constrained to have mean zero so the mean level of the plot is not of interest. There is over prediction at the top end of predicted values which could be a good avenue to start exploring options for model improvement in the future.

```{r}
#| echo: false

train <- train %>% mutate(residuals=residuals(lmod), linpred=predict(lmod))

gdf <- group_by(train, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))

diagdf <- summarise(gdf, residuals=mean(residuals), linpred=mean(linpred))

ggplot(diagdf, aes(x=linpred, y=residuals)) + geom_point() + xlab('linear prediction') + ylab('deviance residuals') + theme_minimal()

```

#### Relationship between Explanatory variables and Residuals

Among the categorical variables only *property_type* appears to have strong association with residuals. We will not address it at this point but again park it as an assessment to be made in future.

```{r}
mod <- aov(residuals ~ location + property_type + bedroom_count, train)
summary(mod)

```

Residuals exhibit a strong patter against *log(active days)*. This is suggestive of a more flexible treatment of active days in the model (we shall opt for splines in a later version of the model).

```{r}
#| echo: false

gdf <- group_by(train, cut(asking_price, breaks=unique(quantile(asking_price, (1:100)/101))))

diagdf <- summarise(gdf, residuals=mean(residuals), asking_price=mean(asking_price))

p1 <- ggplot(diagdf, aes(x=log(asking_price), y=residuals)) + geom_point() + geom_smooth() + xlab('log(asking price)') + ylab('deviance residuals') + theme_minimal()

gdf <- group_by(train, cut(active_days, breaks=unique(quantile(active_days, (1:100)/101))))

diagdf <- summarise(gdf, residuals=mean(residuals), active_days=mean(active_days))

p2 <- ggplot(diagdf, aes(x=log(active_days), y=residuals)) + geom_point() + geom_smooth() + xlab('log(active days)') + ylab('deviance residuals') + theme_minimal()

p1 | p2
```

#### Unusual points
Examining the leverages, there don't seem to be very unusual points that warrant further analysis. Unlike OLS residuals there is no reason to expect normality in this case and so absence of linearity is not of concern.
```{r}
#| echo: false
halfnorm(hatvalues(lmod))
grid()

```

### Goodness of fit

A preliminary examination of how well the model fits the data can be performed by visualising predicted probabilities against observed propertions of price reductions in the data. <br/>
When we make a prediction with probability *p*, we would hope that the event occurs in practice with that proportion.

```{r}
#| echo: false

train <- train %>% mutate(predprob=predict(lmod, type='response'))
gdf <- group_by(train, cut(linpred, breaks=unique(quantile(linpred, (1:100)/101))))
hldf <- summarise(gdf, y=sum(is_5percent_reduced), ppred=mean(predprob), count=n()) %>% mutate(se.fit=sqrt(ppred*(1-ppred)/count))

ggplot(hldf, aes(x=ppred, y=y/count, ymin=y/count-2*se.fit, ymax=y/count+2*se.fit)) + geom_point() + geom_linerange(colour=grey(0.75)) + geom_abline(intercept=0, slope=1) + xlab("Predicted probability") + ylab("Observed proportion")

```
There is no consistent deviation from what is expected and y=x line is contained in the bounds of majority of the data bins. <br/>
A reason to look at the fit visually is that numerical summaries which emulate $R^2$ akin to OLS generally report very low numbers due to bounded response and MLE estimation not explicitly targeting variance of the data.

```{r}
#| echo: false
n <- nrow(train)

cat(paste("Nagelkerke's R^2: ", round((1-exp((lmod$dev-lmod$null)/n))/(1-exp(-lmod$null/n)), 3)))

```
As explained above this value is small and this is partly expected. This does not however rule out further model improvements like including additional informative explanatory variables and changing the functional forms in existing model (as was observed with *log(active days)*).

#### Sensitivity specificity tradeoff
The model can also be used to predict the outcome for each property in the dataset (we've separated a test set already). However, using a 0.5 probability threshold may not be appropriate and we can look at sensitivity/specificity trade off to make cost judgement on model classification outputs. At the default 0.5 threshold, the classification table looks as below.

```{r}
#| echo: false

train <- train %>% mutate(predout=ifelse(predprob < 0.5, "no", "yes"), is_reduced=ifelse(is_5percent_reduced < 0.5, "no", "yes"))

xtabs(~ is_reduced + predout, train)

roc_obj <- roc(train$is_5percent_reduced, train$predprob)
plot(roc_obj$thresholds, roc_obj$sensitivities, type = "l", lty=1,
     xlab = "thresholds", ylab = "Sensitivity",
     main = "Sensitivity-Specificity Tradeoff")
lines(roc_obj$thresholds, roc_obj$specificities, lty=3,
     xlab = "thresholds", ylab = "Specificity")
legend("right", legend = c("Sensitivity", "Specificity"), lty = c(1, 3))
grid()

train <- train %>% mutate(predout=ifelse(predprob < 0.35, "no", "yes"), is_reduced=ifelse(is_5percent_reduced < 0.5, "no", "yes"))

xtabs(~ is_reduced + predout, train)

```

The selection of threshold is going to be biased to favour Sensitivity since there may be desire to be precise about revisions. For now we just balance the 2 attributes and observe that upon choosing a reasonable threshold of *p=0.35* the classification table already looks much better.


## Conclusion
There is much scope for improvement in the current model. <br/>
A few points identified in the analysis above are:

* Greater overprediction at higher predicted values
* Property type is correlated with residuals
* log(active days) has a 2nd order like pattern with residuals

Very fundamentally though, the problem of predicting price reduction is tricky, there are a lot of subjective factors at play with each property along with market related factors. <br/>
This exercise is but a small step in understanding this phenomenon and it definitely has indicated that more information is required to enhance accuracy of the model.
