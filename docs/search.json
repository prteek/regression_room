[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Prateek",
    "section": "",
    "text": "I am a Lead Data Scientist at Bricklane Technologies. When not dabbling in data analysis, I enjoy spending time reading or running."
  },
  {
    "objectID": "posts/price_revision/index.html",
    "href": "posts/price_revision/index.html",
    "title": "Analysis of price reductions",
    "section": "",
    "text": "In real estate universe, often listings on the market experience updates due to either changes to property specific information or due to an influence of markets.  The market influences prices of listings in major way and can lead to price increase or decrease post original advertisement. Of interest to a real investment company are price revisions that are &lt; 0, i.e. where prices of listings decrease. Such properties can offer immediate improvement to rental yield investment cases and shorter times to close due to comparatively lower demand.\nThe current analysis aspires to be able to understand where and based on what factors could price changes have happened using a historic listings dataset. This is to validate the analysis against subject matter knowledge and also expand the current understanding about price revision behaviours in the market.  A better understanding of factors associated with price reductions can help drive strategic decisions e.g. if prices are more likely to be reduced at a particular location in recent data then the market may be experiencing a contraction there.\nThe analysis also extends to prediction of probability of price reduction based on most informative inputs, to be able to have a mechanism where operations teams can prioritise negotiations and due diligence for such properties to benefit from the possibility of their weaker market demand."
  },
  {
    "objectID": "posts/price_revision/index.html#introduction",
    "href": "posts/price_revision/index.html#introduction",
    "title": "Analysis of price reductions",
    "section": "",
    "text": "In real estate universe, often listings on the market experience updates due to either changes to property specific information or due to an influence of markets.  The market influences prices of listings in major way and can lead to price increase or decrease post original advertisement. Of interest to a real investment company are price revisions that are &lt; 0, i.e. where prices of listings decrease. Such properties can offer immediate improvement to rental yield investment cases and shorter times to close due to comparatively lower demand.\nThe current analysis aspires to be able to understand where and based on what factors could price changes have happened using a historic listings dataset. This is to validate the analysis against subject matter knowledge and also expand the current understanding about price revision behaviours in the market.  A better understanding of factors associated with price reductions can help drive strategic decisions e.g. if prices are more likely to be reduced at a particular location in recent data then the market may be experiencing a contraction there.\nThe analysis also extends to prediction of probability of price reduction based on most informative inputs, to be able to have a mechanism where operations teams can prioritise negotiations and due diligence for such properties to benefit from the possibility of their weaker market demand."
  },
  {
    "objectID": "posts/price_revision/index.html#data",
    "href": "posts/price_revision/index.html#data",
    "title": "Analysis of price reductions",
    "section": "Data",
    "text": "Data\nThe data is derived using 2 different data files listings.csv and revisions.csv.  These files individually contain basic attributes of properties and revised prices (both increased in decreased).\nThese are cleaned up for sensible choices of attributes in listing data and price revisions. There are no missing values and the data values are checked for the suitability of application. The script to cleanup and prepare data can be found here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nlisting_id\nlisting_date\nlisting_type\nproperty_type\ntenure\nbedroom_count\nlocation\nasking_price\nrevision_date\nactive_days\nis_5percent_reduced\n\n\n\n\n15526443\n2021-12-16\nbuy\nflat\nleasehold\n2\nGloucester\n155000\n2021-12-16\n0\nno\n\n\n15528531\n2021-12-16\nbuy\nhouse\nfreehold\n3\nCoventry\n225000\n2022-01-11\n0\nno\n\n\n15542023\n2021-09-07\nbuy\nhouse\nfreehold\n2\nReading\n325000\n2021-10-16\n0\nno\n\n\n15550261\n2021-12-22\nbuy\nhouse\nfreehold\n3\nSheffield\n270000\n2022-05-11\n0\nno\n\n\n15557544\n2021-12-22\nbuy\nhouse\nfreehold\n4\nWarrington\n299950\n2022-01-25\n0\nno\n\n\n15565980\n2021-12-24\nbuy\nhouse\nfreehold\n2\nWatford\n450000\n2022-02-25\n0\nno\n\n\n\n\n\nThe columns are self explanatory and the thing worth noting is that active days indicate the time elapsed from listing_date to revision_date.\nWe’ve converted bedroom_count to a qualitative type, since bedroom count has a highly non linear effect on prices and other attributes of property (1-&gt;2 != 2 -&gt; 3).  Additionally, there are only a few different bedroom counts in the data and it could be analysed without treating this information as quantitative.\n\nEDA\nThere are unknown values in tenure which should likely be either of the other 2 categories. The proportion of these listings is about 8% in the data so we cannot drop these listings since their share is quite large.  Additionally, most often flats are observed to be leasehold (which is sensible given that land is not definitively owned) and houses are observed to be freehold. Thus suggesting a strong correlation between tenure and property_type, which means we can drop this column from analysis and use just the property_type for both pieces of information.\n\n\n\n\n\ntenure\nproperty_type\ncounts\n\n\n\n\nfreehold\nflat\n33\n\n\nfreehold\nhouse\n40117\n\n\nleasehold\nflat\n17176\n\n\nleasehold\nhouse\n21\n\n\nunknown\nflat\n456\n\n\nunknown\nhouse\n4689\n\n\n\n\n\nWe do not find a clear relationship between asking_price and properties that have had 5% reduction in prices. It is still possible for there to be some relationship but it is perhaps confounded by location or property_type etc.  We expect that inclusion of location and other factors in the model will clarify effect of asking_price on revisions.  Similar conclusion for active_days. Due to both of these variables having a long tail, these were log transformed and for the rest of the analysis we shall apply the same treatment.\n\n\n\n\n\n\n\n\n\nThere are locations which have had a higher share of properties reduced and this is encouraging as conditional on location effect other variables may be different.  We observe some association between reductions and bedroom count and property type too.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe proportion of properties undergoing price reductions is fairly (mean) stable over time, across almost all locations. We shall not focus on the time component in the rest of the analysis for simplification."
  },
  {
    "objectID": "posts/price_revision/index.html#model",
    "href": "posts/price_revision/index.html#model",
    "title": "Analysis of price reductions",
    "section": "Model",
    "text": "Model\n\nInference\n\n\nis_5percent_reduced ~ property_type + bedroom_count + log(asking_price) + \n    log(active_days) + log(active_days) + location\n\n\nNull deviance: 66829\n\n\nModel deviance: 60479\n\n\nNull df - Model df: 39\n\n\nP-Value X^2: 0\n\n\nEven without a formal test we know that a chi-sq distribution with n degrees of freedom should have an expected value of n. In our case (66829-60479=6350) is much larger than DoF: 39, so we can reject the hypothesis that model is not significantly different from the null model with no explanatory variables.  The chi-square p value suggests the same and we can continue analysing effects captured by the model. \n\n\nEffect of asking price and active days\nBoth of these variables have shapes characteristic of a log transform which was applied previously. It can be observed that asking_price increasing in the lower end significantly affects the likelihood of price reduction. This is reflective of the fact that on a lower baseline, a change of price can easily materialise into a 5% difference as opposed to at higher end of price where 5% reduction requires a very large £ value reduction. \n\n\nWarning in log(x): NaNs produced\nWarning in log(x): NaNs produced\n\n\n\n\n\n\n\n\n\nactive_days does not seem to strongly affect probability of price reduction across its range of values. Towards the initial range of values, the effect seems to be similar to asking_price. The small magnitude of change in probabilities makes the judgement of significance of this variable ambiguos from visual inspection of this single example.  We can test individual explanatory variables by fitting models that drop each variable once and compute the difference in deviance observed.\n\ndrop1(lmod, test=\"Chi\")\n\nSingle term deletions\n\nModel:\nis_5percent_reduced ~ property_type + bedroom_count + log(asking_price) + \n    log(active_days) + log(active_days) + location\n                  Df Deviance   AIC    LRT  Pr(&gt;Chi)    \n&lt;none&gt;                  60479 60559                     \nproperty_type      1    60530 60608   51.4 7.718e-13 ***\nbedroom_count      5    60698 60768  219.3 &lt; 2.2e-16 ***\nlog(asking_price)  1    60677 60755  198.3 &lt; 2.2e-16 ***\nlog(active_days)   1    65525 65603 5046.2 &lt; 2.2e-16 ***\nlocation          31    61157 61175  677.8 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA formal evaluation suggests that all of the explanatory variables are significant. This clarifies confusion around log(active_days). This is in contrast to what was observed during EDA, suggesting a confounding effect due to other variables in the model.\n\n\nEffect of location\nThe plot below gives a comparative assessment of which locations are associated with higher odds of price reduction. This is useful since the summary strips away effects of other explanatory variables and allows a fair comparison between locations (unlike the plot in EDA).  For example, Hull which appeared mid-table in EDA is now near the top. One prominent reason is that Hull has the lowest average asking_price (~200,000£) which as seen above, is associated with a very low probability of price reduction. This fact confounds the effect of location itself which is now revealed.  The overall result may be combined with other data sources to affirm market level behaviours which can be useful for internal know how and targeted acquisitions.  As an example in the chart, places with high odds of reduction:\n\nSheffield\nHull\nMansfield\nWarrington\n\nAre all clustered together in the north not very far from each other. Indicative of a broader market behaviour of North UK.\n\n\n\n\n\n\n\n\n\n\n\nDiagnostics\n\nRelationship between Predicted values and Residuals\nWe construct residual plot by gruping residuals into bins where bins are based on similar predictor values. The choice of number of bins is arbitrary and is made to ensure that we have roughly 500 observations per bin. \nThe deviance residuals are not constrained to have mean zero so the mean level of the plot is not of interest. There is over prediction at the top end of predicted values which could be a good avenue to start exploring options for model improvement in the future.\n\n\n\n\n\n\n\n\n\n\n\nRelationship between Explanatory variables and Residuals\nAmong the categorical variables only property_type appears to have strong association with residuals. We will not address it at this point but again park it as an assessment to be made in future.\n\nmod &lt;- aov(residuals ~ location + property_type + bedroom_count, train)\nsummary(mod)\n\n                 Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nlocation         31     41   1.339   1.112    0.306    \nproperty_type     1     30  30.363  25.214 5.15e-07 ***\nbedroom_count     5      9   1.845   1.532    0.176    \nResiduals     49962  60164   1.204                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nResiduals exhibit a strong patter against log(active days). This is suggestive of a more flexible treatment of active days in the model (we shall opt for splines in a later version of the model).\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nUnusual points\nExamining the leverages, there don’t seem to be very unusual points that warrant further analysis. Unlike OLS residuals there is no reason to expect normality in this case and so absence of linearity is not of concern.\n\n\n\n\n\n\n\n\n\n\n\n\nGoodness of fit\nA preliminary examination of how well the model fits the data can be performed by visualising predicted probabilities against observed propertions of price reductions in the data.  When we make a prediction with probability p, we would hope that the event occurs in practice with that proportion.\n\n\n\n\n\n\n\n\n\nThere is no consistent deviation from what is expected and y=x line is contained in the bounds of majority of the data bins.  A reason to look at the fit visually is that numerical summaries which emulate \\(R^2\\) akin to OLS generally report very low numbers due to bounded response and MLE estimation not explicitly targeting variance of the data.\n\n\nNagelkerke's R^2:  0.162\n\n\nAs explained above this value is small and this is partly expected. This does not however rule out further model improvements like including additional informative explanatory variables and changing the functional forms in existing model (as was observed with log(active days)).\n\nSensitivity specificity tradeoff\nThe model can also be used to predict the outcome for each property in the dataset (we’ve separated a test set already). However, using a 0.5 probability threshold may not be appropriate and we can look at sensitivity/specificity trade off to make cost judgement on model classification outputs. At the default 0.5 threshold, the classification table looks as below.\n\n\n          predout\nis_reduced    no   yes\n       no  26866  3685\n       yes 11634  7815\n\n\nSetting levels: control = 0, case = 1\n\n\nSetting direction: controls &lt; cases\n\n\n\n\n\n\n\n\n\n          predout\nis_reduced    no   yes\n       no  21451  9100\n       yes  8009 11440\n\n\nThe selection of threshold is going to be biased to favour Sensitivity since there may be desire to be precise about revisions. For now we just balance the 2 attributes and observe that upon choosing a reasonable threshold of p=0.35 the classification table already looks much better."
  },
  {
    "objectID": "posts/price_revision/index.html#conclusion",
    "href": "posts/price_revision/index.html#conclusion",
    "title": "Analysis of price reductions",
    "section": "Conclusion",
    "text": "Conclusion\nThere is much scope for improvement in the current model.  A few points identified in the analysis above are:\n\nGreater overprediction at higher predicted values\nProperty type is correlated with residuals\nlog(active days) has a 2nd order like pattern with residuals\n\nVery fundamentally though, the problem of predicting price reduction is tricky, there are a lot of subjective factors at play with each property along with market related factors.  This exercise is but a small step in understanding this phenomenon and it definitely has indicated that more information is required to enhance accuracy of the model."
  },
  {
    "objectID": "posts/movies_getting_longer/index.html",
    "href": "posts/movies_getting_longer/index.html",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "",
    "text": "I recently came across an article posing a question:\n\nAre blockbusters getting (reely) longer?\n\nFueled by the buzz around Christopher Nolan’s Oppenheimer being his longest movie (just over 3 hours), I decided to explore this question using publicly available data.\nAs an exercise in inference this question can be answered with some analysis. I fetched top grossing movies in each year from boxoffice mojo and movie details from imdb and combined them to create the following dataset.\ndata\nThe code to download data is also in this very same .qmd file"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#introduction",
    "href": "posts/movies_getting_longer/index.html#introduction",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "",
    "text": "I recently came across an article posing a question:\n\nAre blockbusters getting (reely) longer?\n\nFueled by the buzz around Christopher Nolan’s Oppenheimer being his longest movie (just over 3 hours), I decided to explore this question using publicly available data.\nAs an exercise in inference this question can be answered with some analysis. I fetched top grossing movies in each year from boxoffice mojo and movie details from imdb and combined them to create the following dataset.\ndata\nThe code to download data is also in this very same .qmd file"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#data",
    "href": "posts/movies_getting_longer/index.html#data",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "Data",
    "text": "Data\n\nyearly_top_movies = read_delim('movies_dataset.csv',\n show_col_types = FALSE)\nhead(yearly_top_movies)\n\n# A tibble: 6 × 3\n  release_year runtime_mins title                       \n         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                       \n1         1990          103 Home Alone                  \n2         1990          127 Ghost                       \n3         1990          181 Dances with Wolves          \n4         1990          119 Pretty Woman                \n5         1990           93 Teenage Mutant Ninja Turtles\n6         1990          135 The Hunt for Red October    \n\n\n\n# Scatterplot of movie runtime over time\nggplot(yearly_top_movies, aes(x = release_year, y = runtime_mins)) +\n  geom_point() + # Scatter points for movie runtimes\n  geom_text(data = annotations, aes(label = title), color = \"red\", nudge_y = 5) + # Annotate specific movies\n  geom_line(data = mean_df, aes(y = runtime_mins), color = \"brown\") + # Line for yearly mean runtimes\n  ggtitle(\"Top Grossing Movies per Year\") +\n  xlab(\"Release Year\") +\n  ylab(\"Runtime (mins)\")"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#analysis",
    "href": "posts/movies_getting_longer/index.html#analysis",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "Analysis",
    "text": "Analysis\n\nComparing recent vs old movies\nTo simplify the analysis we can consider more recent releases (2013-2023) and compare them to releases from much older timeframe (1990-2000).  The runtime distributions for movies in these 2 categories is different although not significantly (visually).\n\ntestset &lt;- yearly_top_movies %&gt;%\n    mutate(\n    label = case_when(\n      release_year &gt;= 2013 ~ \"recent\",\n      release_year &lt;= 2000 ~ \"old\",\n      TRUE ~ \"other\"\n    )) %&gt;% filter(label != \"other\")\n\nggplot(testset, aes(x=runtime_mins, fill=label)) + geom_density(alpha=0.3)\n\n\n\n\n\n\n\n\nNow we can compare runtimes between two periods. It would have been a straightforward exercise in testing the hypothesis of difference of means using a t-test had the distributions of runtimes been more normally distributed. \nAdditionally, the cutoff chosen for old (&lt;= 2000) and recent (&gt;=2013) are arbitrary. We would ideally want a more comprehensive statement about movie runtimes increasing over the years. \nNeverthless, as a preliminary (and simple) analysis we can use bootstrap to compare means of runtimes in the above groups.\n\nlibrary(boot)\n\n# Define a function to calculate the difference of means\ndiff_mean &lt;- function(data, indices) {\n  # Resample the data\n  resampled_data &lt;- data[indices, ]\n\n  # Calculate means for each group\n  mean_group1 &lt;- mean(resampled_data$runtime_mins[resampled_data$label == \"old\"])\n  mean_group2 &lt;- mean(resampled_data$runtime_mins[resampled_data$label == \"recent\"])\n\n  # Return the difference of means\n  return(mean_group2 - mean_group1)\n}\n\nset.seed(123)\nbootstrap_results &lt;- boot(data = testset, statistic = diff_mean, R = 1000)\n\n# Print bootstrap results\nprint(bootstrap_results)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = testset, statistic = diff_mean, R = 1000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 9.697248 0.06394496    3.174053\n\n# Plot bootstrap distribution\nplot(bootstrap_results)\n\n\n\n\n\n\n\n# Calculate confidence interval for the difference of means\nci &lt;- boot.ci(bootstrap_results, type = \"perc\")\nprint(ci)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap_results, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 3.502, 15.736 )  \nCalculations and Intervals on Original Scale\n\n\nIf we bootstrap the mean of movie times and take the difference between means of recent release years from old, there is evidence that blockbuster movies are getting longer in recent years, and the difference can be up to 10 min on average and can be expected to be between 4 min and 16 min. \nSo on your next visit to the theatre make sure to get some extra popcorn !!!\n\n\nEstimating effect of time\nIn the want of better precision in identifying the effect of time on movie runtimes, we can formulate a regression problem and estimate the effect of year.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe plot above partly confirms our intuition about the increasing trend of runtime.\n\nlmod &lt;- lm(runtime_mins ~ release_year, yearly_top_movies)\nsummary(lmod)\n\n\nCall:\nlm(formula = runtime_mins ~ release_year, data = yearly_top_movies)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.369 -19.617   0.488  16.422  77.834 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -636.1435   271.4520  -2.343  0.01969 * \nrelease_year    0.3791     0.1353   2.802  0.00537 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.42 on 335 degrees of freedom\nMultiple R-squared:  0.0229,    Adjusted R-squared:  0.01999 \nF-statistic: 7.852 on 1 and 335 DF,  p-value: 0.005372\n\n\nThe regression above supports the hypothesis that release_year has positively contributed to movie runtimes at approximately 0.38 min/year.  Considering our previous finding on average between 1995 and 2018 the runtime increased by approx. 10 min. From the regression estimate the increase is 0.3791 * (2018-1995) = 8.7 min. \nWe can certainly run some diagnostics to be sure of our model.\n\n\n\n\n\n\n\n\n\nThe residuals appear to have no pattern with fitted values and also have fairly constant variance. There are no significantly high leverage points or outliers.  However, the errors are not quite normally distributed.  This can be ignored by relying on large sample size and the fact that other diagnostics are fine.  But, we shall check if just in case any transformation of runtime_mins may help.\n\nsuppressPackageStartupMessages(library(MASS))\nboxcox(lmod, lambda=seq(-1,1,by=0.1))\n\n\n\n\n\n\n\n\nThe Box-Cox transformation check above hints that there may be some benefit if we used log-transformation (since lambda approximately 0). This thread does not materialise into anything meaningful as upon transformation, the diagnostics do not change materially and the effect size of release_year is practically the same (0.32% per year, 0.38 min/year on base of 119 min). So we will not use any transformation in our model.\nThere may also be some degree of autocorrelation among residuals due to the fact that we’re working with timeseries data. We can test for this autocorrelation and handle it if necessary.\n\n\nAuto-correlation among residuals:  0.01\n\n\n\n    Durbin-Watson test\n\ndata:  runtime_mins ~ release_year\nDW = 1.982, p-value = 0.4126\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nThere doesn’t appear to be significant autocorrelation and other model diagnotics appear to be fine."
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#conclusion",
    "href": "posts/movies_getting_longer/index.html#conclusion",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "Conclusion",
    "text": "Conclusion\nThere has been a rather steady increase of 0.38 min/year (on average) in movie runtimes as per the dataset we have used. In effect expect to be seated for about 10 more minutes in your favourite blockbuster compared to when you went to movies as a kid."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html",
    "href": "posts/user_behaviour_analysis/index.html",
    "title": "User behaviour analysis",
    "section": "",
    "text": "Duolingo have been making waves with their customer success stories and strong (and growing) user base. They seem to be taking their metrics seriously and have done a great job explaining their approach to understanding user behaviour in this (Gustafson 2024).  In the current blog, I wanted to apply their methodology and use it for predicting future customer behaviours. \nThe problem statement driving this analysis is simple. Duolingo want to predict their future daily active users (DAU) with some control handles that they can manipulate to improve those numbers e.g. send reminders to at risk weekly active users or reward campaigns to at risk monthly active users.  They may be able to run (or have already run) experiments identifying the effects of these campaigns and reminder in terms of % of people that respond to the treatment and revert back to being daily active.  This allows them to efficiently allocate resources and maintain a healthy user base that eventually materialises into paying customers."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#introduction",
    "href": "posts/user_behaviour_analysis/index.html#introduction",
    "title": "User behaviour analysis",
    "section": "",
    "text": "Duolingo have been making waves with their customer success stories and strong (and growing) user base. They seem to be taking their metrics seriously and have done a great job explaining their approach to understanding user behaviour in this (Gustafson 2024).  In the current blog, I wanted to apply their methodology and use it for predicting future customer behaviours. \nThe problem statement driving this analysis is simple. Duolingo want to predict their future daily active users (DAU) with some control handles that they can manipulate to improve those numbers e.g. send reminders to at risk weekly active users or reward campaigns to at risk monthly active users.  They may be able to run (or have already run) experiments identifying the effects of these campaigns and reminder in terms of % of people that respond to the treatment and revert back to being daily active.  This allows them to efficiently allocate resources and maintain a healthy user base that eventually materialises into paying customers."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#methodology",
    "href": "posts/user_behaviour_analysis/index.html#methodology",
    "title": "User behaviour analysis",
    "section": "Methodology",
    "text": "Methodology\nFirst we shall model the process as a Markov chain to see how well the approach predicts future user counts.  Subsequently, to validate MC approach and rethink the problem setup in want of simplification, we shall look to model the time-series as a regression problem. We shall do this respecting the mechanics of transition and see how far this can get us in terms of analysing and controlling the process.\nAt day d (d = 1, 2, … ) of a user’s lifetime, the user can be in one of the following 7 (mutually-exclusive) states: \n\nnew : learners who are experiencing Duolingo for the first time ever\ncurrent : learners active today, who were also active in the past week\nreactivated : learners active today, who were also active in the past month (but not the past week)\nresurrected : learners active today, who were last active &gt;30 days ago\nat_risk_wau (at risk weekly active users) : learners who have been active within the past week, but not today\nat_risk_mau (at risk monthly active users) : learners who were active within the past month, but not the past week\ndormant : learners who have been inactive for at least 30 days\n\nA brief overview of how these states are related is below:\n\n\n\nUser states and transitions\n\n\nThe transition acronyms are defined below:\n\nNURR : New User Retention Rate (The proportion of day 1 learners who return on day 2)\nCURR : Current User Retention Rate (This is not a 100%. Not all of current users stay active every day since people forget to complete a lesson, or have technical difficulties, or just want to take a break)\nRURR : Reactivated User Retention rate (The proportion of at_risk_mau who became active today)\nSURR : Resurrected User Retention rate (The proportion of dormant users who became active today)\niWAURR : The proportion of at_risk_wau users who became active today\n\nAs is evident, these quantities (along with others) are what will become the elements of Transition matrix (M), that we discuss next.\nIn keeping with the definition chart above, the next step is to consider user behaviour as a Markov chain.  Let M be a transition matrix associated with this Markov process: m(i, j) = P(s_j | s_i) are the probabilities that a user moves to state s_j right after being at state s_i. The matrix M is learned from the historical data.\nAssuming that user behavior is stationary (independent of time), the matrix M fully describes the states of all users in the future.\nSuppose that the vector u_0 of length 7 contains the counts of users in certain states on a given day (say day 0). According to the Markov model, on the next day (day 1), we expect to have the following number of users (u_1) in respective states:\n\n\n\nState transition counts estimation\n\n\nApplying this multiplication recursively, we can derive the number of users in any states on any arbitrary day t &gt; 0 in the future (call this vector u_t).\nNow, having u_t calculated, we can determine DAU, WAU and MAU values on day t:\n\nDAU_t = #New_t + #Current_t + #Reactivated_t + #Resurrected_t\nWAU_t = DAU_t + #AtRiskWau_t\nMAU_t = DAU_t + #AtRiskWau_t + #AtRiskMau_t\n\nFinally, here’s the algorithm outline:\n\nFor each prediction day t = 1, …, T, calculate the expected number of new users #New_1, …, #New_T.\nFor each lifetime day of each user, assign one of the 7 states.\nCalculate the transition matrix M from the historical data.\nCalculate initial state counts u_0 corresponding to day t=0.\nRecursively calculate u_{t+1} = M^t * u_0.\nCalculate DAU, WAU, and MAU for each prediction day t = 1, …, T."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#implementation",
    "href": "posts/user_behaviour_analysis/index.html#implementation",
    "title": "User behaviour analysis",
    "section": "Implementation",
    "text": "Implementation\n\nData\nWe use a simulated dataset based on historical data of a SaaS app.  The data is available here and contains three columns: user_id, date, and registration_date.\nEach record indicates a day when a user was active. \n\ndau_data &lt;- read_csv(\"dau_data.csv\", show_col_types = FALSE)\nkable(head(dau_data))\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\nd8c465ab-e9fd-5edd-9e4e-c77094700cb5\n2020-10-01\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-01\n2020-08-25\n\n\nbfeac474-5b66-566f-8654-262bb79c873e\n2020-10-01\n2020-05-31\n\n\nd32fcac5-122c-5463-8aea-01b39b9ad0bb\n2020-10-01\n2020-09-30\n\n\nc1ece677-e643-5bb3-8701-f1c59a0bf4cd\n2020-10-01\n2020-09-05\n\n\nba6750c9-9b60-58dc-8d40-504e7375249e\n2020-10-01\n2020-09-29\n\n\n\n\n\n\n\nTotal users: 51480\n\n\nDate range: 2020-10-01 to 2023-10-31\n\n\nThis is how DAU timeseries looks\n\n\n\n\n\n\n\n\n\n\n\nFuture new user count model\nTo even begin thinking about transitions on a future date, we first need to acknowledge the fact that new user counts will be a significant missing piece of information in the analysis.  To get around this limitation, we shall first build a new user count prediction model that need not be very accurate (since in a realistic setting new user counts can also be a parameter that can be changed).\nThe information on new users on any given date is inherent in registration_date in the data.\n\nnew_users &lt;- dau_data %&gt;%\n                filter(date==registration_date) %&gt;%\n                group_by(date) %&gt;%\n                summarise(new_user_count=n()) %&gt;%\n                mutate(\n                    year = year(date),\n                    week_of_year = isoweek(date),  # ISO 8601 week (Monday starts the week)\n                    month = as.factor(month(date, label = TRUE, abbr = TRUE)),  # Month as abbreviated name\n                    day_of_week = wday(date)\n\n                    )\n\nlmod &lt;- glm.nb(new_user_count ~ year * bs(week_of_year,4) + month + day_of_week , data=new_users)\n\n# summary(lmod)\n\nAfter some experimentation, the model setup to predict future counts of new users is as above.  The counts are assumed Poisson but with a higher variance than normal due to spikes in the data and hence a negative binomial model has been used.  It was imperative that we used a robust version of GLM to be able to get relatively constant variance of errors, that were being caused due to outliers in user count towards beginning of every year.  The model appears to be doing a satisfactory job of predicting counts of new users and we consider this acceptable for further analysis.\n\n\n\n\n\n\n\n\n\n\n\nAssigning states\nThere needs to be an assignment of one of the 7 states (mentioned earlier) against each date for a user. This will help identify transitions made between states and populate the Transition matrix. However, the data only consists of records of active days, we need to explicitly extend them and include the days when a user was not active. In other words, instead of this table of records:\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n1234567\n2023-01-01\n2023-01-01\n\n\n1234567\n2023-01-03\n2023-01-01\n\n\n\nwe’d like to get a table like this:\n\n\n\nuser_id\ndate\nis_active\nregistration_date\n\n\n\n\n1234567\n2023-01-01\nTRUE\n2023-01-01\n\n\n1234567\n2023-01-02\nFALSE\n2023-01-01\n\n\n1234567\n2023-01-03\nTRUE\n2023-01-01\n\n\n1234567\n2023-01-04\nFALSE\n2023-01-01\n\n\n…\n…\n…\n…\n\n\n1234567\n2023-10-31\nFALSE\n2023-01-01\n\n\n\nAnd using this table, subsequently estimate states using simple rules (mentioned in definitions above) for each user_id, corresponding to each day in the data.\nThe volume of data at our disposal makes this a challenging task to run conveniently on a small machine, so the state determination is performed offline and the code to do it can be found here\n\nstates &lt;- read_csv('states.csv', show_col_types = FALSE)\nkable(states %&gt;%\n        filter(grepl('269b7f13-a509-5174-85cb', user_id)) %&gt;%\n        filter(between(date, as.Date('2020-10-18'), as.Date('2020-10-30')))\n        )\n\n\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nis_active\nstate\nregistration_date\n\n\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-18\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-19\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-20\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-21\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-22\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-23\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-24\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-25\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-26\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-27\nFALSE\nat_risk_mau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-28\nTRUE\nreactivated\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-29\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-30\nTRUE\ncurrent\n2020-08-25\n\n\n\n\n\n\n\nCalculating transition matrix\nWe need a flexible approach to calculate transition matrix over any arbitrary period of time. This is because it is likely that transition matrix changes over time as user preferences and product itself evolves. \n\nget_transition_matrix &lt;- function(states_slice) {\n        M &lt;- states_slice %&gt;%\n            group_by(user_id) %&gt;%\n            arrange(date) %&gt;%\n            mutate(\n                state_to = lead(state),\n                state_from = state\n            ) %&gt;%\n            ungroup() %&gt;%\n            # Remove the last row since user doesn't transition to anywhere from that\n            filter(!is.na(state_to)) %&gt;%\n            # Count transitions per user to avoid counting transitions across users\n            group_by(user_id, state_from, state_to) %&gt;%\n            summarise(transition_count = n()) %&gt;%\n            ungroup() %&gt;%\n            # Sum all the transitions over all users\n            group_by(state_from, state_to) %&gt;%\n            summarise(transition_count = sum(transition_count)) %&gt;%\n            ungroup() %&gt;%\n            # Generates a wide form matrix from long form\n            pivot_wider(names_from = state_to, values_from = transition_count, values_fill = list(transition_count = 0)) %&gt;%\n            # Normalizes matrix of counts so that rows sum to 1\n            mutate(row_sum = rowSums(dplyr::select(., -state_from))) %&gt;%\n            mutate(across(-state_from, ~ . / row_sum)) %&gt;%\n            dplyr::select(-row_sum) %&gt;%\n            # Generates ordered matrix where row and column name orders match\n            mutate(new = 0) %&gt;% # To get consistent matrix since 'new' is missing from state_to\n            column_to_rownames(\"state_from\") %&gt;%\n            .[order(rownames(.)), order(colnames(.))]\n    return(M)\n}\nEssentially we seek to calculate a matrix which has state_from in its rows state_to as its column and fraction of transitions as values. All the values in a row must sum to 1 obeying the fact that from a given state a user will end up in one of the 7 states.\n\nM = get_transition_matrix(states %&gt;%\n                            filter(between(date, start_date, end_date)))\n\nkable(M)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat_risk_mau\nat_risk_wau\ncurrent\ndormant\nnew\nreactivated\nresurrected\n\n\n\n\nat_risk_mau\n0.9520158\n0.0000000\n0.0000000\n0.0384384\n0\n0.0093866\n0.0001591\n\n\nat_risk_wau\n0.1308719\n0.7663644\n0.0982922\n0.0000000\n0\n0.0044715\n0.0000000\n\n\ncurrent\n0.0000000\n0.1487694\n0.8512306\n0.0000000\n0\n0.0000000\n0.0000000\n\n\ndormant\n0.0000000\n0.0000000\n0.0000000\n0.9996200\n0\n0.0000000\n0.0003800\n\n\nnew\n0.0000000\n0.4836773\n0.5163227\n0.0000000\n0\n0.0000000\n0.0000000\n\n\nreactivated\n0.0000000\n0.6353907\n0.3646093\n0.0000000\n0\n0.0000000\n0.0000000\n\n\nresurrected\n0.0000000\n0.6848495\n0.3151505\n0.0000000\n0\n0.0000000\n0.0000000\n\n\n\n\n\nAs a sanity check, we can analyse the matrix itself.  It makes sense that current -&gt; current transition value is high. This means that active users tend to continue being active with high probability or otherwise become at_risk_wau.  It also seems sensible that there is no state from which one could transition to new. Apparantly quite a high proportion of dormant users stay dormant which could be one of the focuses of resurrection campaigns in such companies.\n\n\nPredicting DAU\nWe start with getting initial state counts for the date on which the dataset ends. This combined with M transition matrix should give us new state counts. We will be mindful to update new_user_count for all the days we would like to predict on. This is done using the prediction model developed earlier.\n\ndata_end_date &lt;- prediction_start_date - 1\nstate0 &lt;- states %&gt;%\n            filter(date==data_end_date) %&gt;%\n            group_by(state) %&gt;%\n            summarise(cnt=n(), .groups='drop')\n\nWe shall define a flexible mechanism to predict DAU, to enable changing prediction horizon. We will also supply a dataframe of new_users to this mechanism so that it may use predicted values for appropriate prediction dates.\n\npredict_dau &lt;- function(M, state0, start_date, end_date, new_users_predicted) {\n    dates &lt;- seq(as.Date(start_date), as.Date(end_date), by = \"day\")\n    # Align state0 name order with transition Matrix name order\n    new_dau &lt;- state0[match(state0$state, rownames(M)), ]$cnt\n    dau_pred &lt;- list()\n\n    for (dt in dates) {\n        new_dau &lt;- as.integer(t(M) %*% new_dau) # Transitions\n        new_users_today &lt;- new_users_predicted %&gt;%\n            filter(date == dt) %&gt;%\n            pull(predicted) %&gt;%\n            as.integer()\n        new_dau[5] &lt;- new_users_today # Today's new users get transitioned next day\n        dau_pred &lt;- append(dau_pred, list(new_dau))\n    }\n\n    df &lt;- data.frame(do.call(rbind, dau_pred))\n    colnames(df) &lt;- rownames(M)\n    df$date &lt;- dates\n    df$dau &lt;- df$new + df$current + df$reactivated + df$resurrected\n    df$wau &lt;- df$dau + df$at_risk_wau\n    df$mau &lt;- df$dau + df$at_risk_wau + df$at_risk_mau\n\n    return(tibble(df))\n}\n\ndau_predicted &lt;- predict_dau(M, state0, prediction_start_date, prediction_end_date, new_users_predicted)\n\np &lt;- (ggplot(dau_data %&gt;%\n        group_by(date) %&gt;%\n        summarise(DAU=n())\n        ) +\ngeom_line(aes(x=date, y=DAU)) +\ngeom_line(data=dau_predicted, aes(x=date, y=dau), colour='red') +\nggtitle('DAU Predictions')\n)\n\n\n\n\n\n\n\n\n\n\nThe prominent contributer to DAU counts are current users, who are aided by addition of new users everyday and the fact that current users stay active with a probability of 85% (from M matrix).  The prediction makes intuitive sense however the magnitude of spike certainly seems quite large.  Observing the shape of predicted DAU, it seems quite strongly influenced by predicted new user count from the model earlier. Additionally, some degree of experimentation with the predictions suggest that about 20% decrease in new user counts bring predicted DAU to comparable levels of previous years (dotted line in the plot)."
  },
  {
    "objectID": "posts/likelihood_primer/index.html",
    "href": "posts/likelihood_primer/index.html",
    "title": "Primer on Likelihood",
    "section": "",
    "text": "Likelihood is a fundamental concept that plays a pivotal role in parameter estimation and model fitting.  Whether one is estimating the parameters of a simple distribution or building complex statistical models, likelihood functions provide a way to quantify how likely a set of observed data is, given specific model parameters.  In this post, I’ll break down the concept of likelihood, explore how it relates to probability, and demonstrate its application in parameter estimation using methods of Maximum Likelihood Estimation (MLE) and Newton Raphson.  This primer will help bring clarity to working with Likelihoods for parameter estimation.  The concepts discussed are deeply inspired by (Faraway 2016) and (George Casella 2002). For a more comprehensive understanding of the subject, I highly recommend reading the full texts."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#introduction",
    "href": "posts/likelihood_primer/index.html#introduction",
    "title": "Primer on Likelihood",
    "section": "",
    "text": "Likelihood is a fundamental concept that plays a pivotal role in parameter estimation and model fitting.  Whether one is estimating the parameters of a simple distribution or building complex statistical models, likelihood functions provide a way to quantify how likely a set of observed data is, given specific model parameters.  In this post, I’ll break down the concept of likelihood, explore how it relates to probability, and demonstrate its application in parameter estimation using methods of Maximum Likelihood Estimation (MLE) and Newton Raphson.  This primer will help bring clarity to working with Likelihoods for parameter estimation.  The concepts discussed are deeply inspired by (Faraway 2016) and (George Casella 2002). For a more comprehensive understanding of the subject, I highly recommend reading the full texts."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#likelihood",
    "href": "posts/likelihood_primer/index.html#likelihood",
    "title": "Primer on Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\n\nDiscrete RV case\nIf we consider there to be a set of independent discrete random variables \\(Y_1,...., Y_n\\) with probability mass function \\(f(y|\\theta)\\) where \\(\\theta\\) is the parameter.  Let \\(\\mathbf{y} = (y_1,....,y_n)^T\\) be the observed values, then the likelihood function for the distribution is:\n\\[\nP(\\mathbf{Y}=\\mathbf{y}) = \\prod_{i=1}^{n} f(y_i|\\theta) = L(\\theta|y)\n\\]\nAs can be seen likelihood is about considering parameter \\(\\theta\\) as random and data \\(\\mathbf{y}\\) as fixed (hence the notation \\(L(\\theta|y)\\)). This is as opposed to a joint density function where \\(\\theta\\) are fixed and \\(y\\) are random.  Effectively though likelihood is still the probability of observed data \\(\\mathbf{y}\\) for a specified value of the parameter(s) i.e. \\(p(\\mathbf{y}|\\theta)\\) but the key distinction is that likelihood changes as \\(\\theta\\) changes.\n\n\nContinuous RV case\nWhen the random variables are continuous (with probability density \\(f(y|\\theta)\\)), in practice one can measure data with limited precision. When a value \\(y_i\\) is observed it effectively indicates an observation in the range \\([y_i^l, y_i^u]\\), where \\(y_i^l\\) and \\(y_i^u\\) are lower and upper bounds of precision limit.  Using which we can reason that probability of observed value is a concept defined over an interval:\n\\[\nP(Y_i=y_i) = P(y_i^l&lt;=y_i&lt;=y_i^u) = \\int_{y_i^l}^{y_i^u} f(u|\\theta) \\, du \\approx f(y_i|\\theta) \\delta_i\n\\]\nsuch that \\(\\delta_i = y_i^u-y_i^l\\), which does not depend on \\(\\theta\\) and is related to the precision of measurement.  And, the likelihood becomes:\n\\[\nL(\\theta|y) \\approx \\prod_{i=1}^{n} f(y_i|\\theta) \\prod_{i=1}^{n}\\delta_i\n\\]\nGiven we know that \\(\\delta_i\\) does not depend on \\(\\theta\\), the last term can be ignored and the likelihood then becomes same as in the discrete case."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#binomial-example",
    "href": "posts/likelihood_primer/index.html#binomial-example",
    "title": "Primer on Likelihood",
    "section": "Binomial example",
    "text": "Binomial example\nLet’s consider a binomially distributed random variable \\(Y \\sim B(n,p)\\) (Note: here \\(p\\) is the parameter of interest).  Its likelihood is:\n\\[\nL(p|y) = \\binom{n}{y}p^y(1-p)^{n-y}\n\\]\nThe maximum likelihood estimate (MLE) of \\(p\\) results in the largest probability of observed data. The value at which the maximum occurs is called the maximum likelihood estimate and is denoted as \\(\\hat{p}\\).\nOften, it is simpler to maximise the log of likelihood function \\(l(\\theta|y) = log(L(\\theta|y))\\), considering that log is a monotonic increasing function so maximising either should result in the exact same estimate \\(\\hat{\\theta}\\).\nTo find the solution for \\(\\hat{p}\\) in our binomial case, we can start by defining the log-likelihood:\n\\[\nl(p|y) = log(\\binom{n}{y}) + ylog(p) + (n-y)log(1-p)\n\\]\nA typical calculus approach to maximising this function would be to take the derivative of this log-likelihood w.r.t. parameters of interest and set it to 0.  Worth noting that the derivative is called score function and denoted as \\(u(\\theta)\\) in general. For binomial we have:\n\\[\nu(p) = \\frac{d}{dp}l(p|y) = \\frac{y}{p} - \\frac{n-y}{1-p}\n\\]\nTo get the maximum likelihood estimate \\(\\hat{p}\\) we solve for \\(u(p) = 0\\) and get \\(\\hat{p} = y/n\\)."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#fisher-information-uncertainty",
    "href": "posts/likelihood_primer/index.html#fisher-information-uncertainty",
    "title": "Primer on Likelihood",
    "section": "Fisher Information (uncertainty)",
    "text": "Fisher Information (uncertainty)\nSome measure of uncertainty is desirable here to be able to rely on the estimate. This is given by Fisher information:\n\\[\nI(\\theta) = var(u(\\theta)) = -\\mathbb{E}[\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T}l(\\theta)]\n\\]\nThe double derivative part here implies the rate of change of score itself.  At (or near) the maximum values of log-likelihood, a large value of \\(I(\\theta)\\) effectively indicates a large rate of change of score. Which means that the maximum is clearly defined (curvature of log-likelihood is large).\nUnder regularity conditions, the maximum likelihood estimator (MLE) of the parameter \\(\\theta\\) , denoted as \\(\\hat{\\theta}\\) , is asymptotically unbiased and normally distributed. That is: \\[\n\\hat{\\theta} \\sim \\mathcal{N}\\left(\\theta, \\frac{1}{I(\\theta)}\\right).\n\\] and this means that \\(var(\\hat{\\theta}) = I^{-1}(\\theta)\\). Which gives us the uncertainty on parameter estimate itself.  This makes intuitive sense, when Fisher information is high (clearly defined maximum and high curvature of log-likelihood), then the uncertainty in parameter estimate should be low.\n\nFisher information for Binomial\nNow, in the Fisher information, the expectation is w.r.t. data. Consider the following for binomial distributed data:\n\\[\\begin{align}\n    \\frac{d}{dp}l(p|y) = \\frac{y}{p} - \\frac{n-y}{1-p} \\\\\n    \\frac{d^2}{dp^2}l(p|y)l(p|y) = -\\frac{y}{p^2} - \\frac{n-y}{(1-p)^2} \\\\\n    \\mathbb{E}[\\frac{d^2}{dp^2}l(p|y)l(p|y)] = \\mathbb{E}[-\\frac{y}{p^2} - \\frac{n-y}{(1-p)^2}] \\\\\n    and, \\mathbb{E}[y] = np \\\\\n    \\mathbb{E}[\\frac{d^2}{dp^2}l(p|y)l(p|y)] = -\\frac{n}{p} - \\frac{n}{(1-p)} \\\\\n    I(p) = - \\mathbb{E}[\\frac{d^2}{dp^2}l(p|y)l(p|y)] = \\frac{n}{p} + \\frac{n}{(1-p)} \\\\\n    I(p) = \\frac{n}{p(1-p)}\n\n\\end{align}\\]\nSo, the larger the \\(n\\) (sample size) the greater is Fisher information and lesser uncertainty about the maxima point of score function. Similarly, for same \\(n\\) more extreme values of \\(p\\) will enable estimation of parameter with greater degree of confidence due to higher Fisher Information. Intuitively, this is because extreme values in data will be observed (lots of 1s or lots or 0s).\n\n\n\n\n\n\n\n\n\n\n\nEffect of Fisher information on log-likelihood\nWe can also illustrate the effect on score (log-likelihood) becoming more defined with higher Fisher information (due to higher \\(n\\)) with same choices as above plot.  Essentially we consider 2 binomial datasets one with \\(n=25, y=10\\) and another with \\(n=50, y=20\\) where \\(y\\) is observed counts of successes.\n\nloglik &lt;- function(x, y, n) lchoose(n,y) + y*log(x) + (n-y)*log(1-x)\n\ncurve(loglik(x,y=10,n=25), 0,1, lty=1, xlab='p', ylab='Log-likelihood',\n        main='Log-likelihood of Binomial distribution')\ncurve(loglik(x,y=20,n=50), 0,1, add=TRUE, lty=2)\ngrid()\nlegend(\"topright\", legend = c(\"n=25\", \"n=50\"), lty = c(1,2))\n\n\n\n\n\n\n\n\nThe maximum can be noticed at \\(p=0.4\\) (equal to \\(n/y\\) in both cases as derived earlier). For larger sample size \\(n=50\\) the curvature of log-likelihood is greater as there is more information available.  For our binomial example we maximised the log-likelihood function analytically earlier. It was a convenient exercise, however it is not always possible to analytically maximise log-likelihood and estimate parameters."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#numerical-estimation",
    "href": "posts/likelihood_primer/index.html#numerical-estimation",
    "title": "Primer on Likelihood",
    "section": "Numerical estimation",
    "text": "Numerical estimation\nTypically numerical optimisation is necessary when estimating parameters using log-likelihood maximisation. The Newton-Raphson method is applied for these purposes. Consider \\(\\theta_0\\) as an initial guess for \\(\\theta\\), then an update is made using: \\[\n\\theta_1 = \\theta_0 - H^{-1}(\\theta_0) J(\\theta_0)\n\\] Where,  \\(H(\\theta) = \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T}l(\\theta)\\) is the Hessian matrix of second derivatives of log-likelihood w.r.t. \\(\\theta\\) .  \\(J(\\theta) = \\frac{d}{d\\theta}l(\\theta)\\) is the Jacobian matrix of derivatives of log-likelihood w.r.t. \\(\\theta\\) .\nThis method works well, provided the log-likelihood is smooth and convex around the maximum and the initial value is reasonably chosen, but is not a guaranteed solution for all maximisation problems (such as when maximum is on boundary of parameter space or when there are several local maximums). \n\nNormal example\nTo develop intuition on this method consider a Normal distribution: \\[\nf(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\]\nIts log-likelihood can be written as \\[\nl(\\mu) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n\\]\nWe calculate the first and second derivatives of log-likelihood\n\\[\\begin{align}\nl'(\\mu) = \\frac{\\partial l(\\mu)}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) \\\\\n\nl''(\\mu) = \\frac{\\partial^2 l(\\mu)}{\\partial \\mu^2} = -\\frac{n}{\\sigma^2}\n\n\\end{align}\\]\nThen we perform the Newton Raphson update\n\\[\\begin{align}\n\\mu^{(t+1)} = \\mu^{(t)} - \\frac{l'(\\mu^{(t)})}{l''(\\mu^{(t)})} \\\\\n\\mu^{(t+1)} = \\mu^{(t)} - \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu^{(t)})}{-\\frac{n}{\\sigma^2}} \\\\\n\\mu^{(t+1)} = \\mu^{(t)} + \\frac{\\sum_{i=1}^n (x_i - \\mu^{(t)})}{n} \\\\\n\\mu^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\end{align}\\]\nThus, in this case, the Newton-Raphson method directly converges to the sample mean \\(\\bar{x}\\) , which is the maximum likelihood estimate (MLE) for \\(\\mu\\).\n\n\nBinomial solution\nLet’s now revert back to our Binomial distribution setting. We can perform log-likelihhod maximisation with Newton Raphson using R. Note that we need to minimize \\(-l(\\theta)\\) because nlm minimizes and doesn’t maximize.\n\nn &lt;- 25\ny &lt;- 10\nf &lt;- function(x) -loglik(x, y, n)\nmle &lt;- nlm(f, 0.5, hessian=TRUE)\n\nWarning in log(x): NaNs produced\n\n\nWarning in nlm(f, 0.5, hessian = TRUE): NA/Inf replaced by maximum positive\nvalue\n\n\nWarning in log(x): NaNs produced\n\n\nWarning in nlm(f, 0.5, hessian = TRUE): NA/Inf replaced by maximum positive\nvalue\n\n\nWe use an initial value of \\(p=0.5\\) and set hessian to TRUE to be able to calculate standard errors of the estimate using it.\nAt the MLE \\(\\hat{\\theta}\\) , the Hessian \\(H(\\hat{\\theta})\\) is often used as an estimate for the Fisher information. (since both are effectively double derivative of log-likelihood). We have \\[\nI(\\hat{\\theta}) \\approx -H(\\hat{\\theta})\n\\] We have seen earlier that the variance of parameter is the inverse of Fisher information, which gives us \\[\n\\text{Var}(\\hat{\\theta}) = I^{-1}(\\theta) \\approx [-H(\\hat{\\theta})]^{-1}\n\\]\nHence, the inverse of the Hessian at the optimum is equal to the variance of the estimate.  Note: - sign is excluded since we’re working with -log-likelihood in nlm\n\ncat(paste(\"Numerical estimate of p:\", round(mle$estimate, 2)))\n\nNumerical estimate of p: 0.4\n\nest &lt;- paste(\"Estimated variance of p:\", round(1/mle$hessian, 4))\nact &lt;- paste(\"Actual variance variance of p:\", round(0.4*(1-0.4)/n, 4))\ncat(paste(est, act, sep='\\n'))\n\nEstimated variance of p: 0.0096\nActual variance variance of p: 0.0096\n\n\nThis is a useful toolkit to have when working with estimations and maximum likelihood."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#wrap-up",
    "href": "posts/likelihood_primer/index.html#wrap-up",
    "title": "Primer on Likelihood",
    "section": "Wrap up",
    "text": "Wrap up\nBy understanding these fundamental concepts, one can gain deeper insights into statistical inference and model fitting, helping to make more informed decisions in real-world applications."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Room",
    "section": "",
    "text": "Teen gambling analysis\n\n\n\n\n\n\nregression\n\n\nANOVA\n\n\n\nAnalysis of factors affecting teen gambling using regression and ANOVA\n\n\n\n\n\nOct 16, 2025\n\n\nPrateek\n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis of price reductions\n\n\n\n\n\n\nlogistic regression\n\n\nprobability\n\n\n\nModelling and analysing probability of house sale price reductions\n\n\n\n\n\nJan 8, 2025\n\n\nPrateek\n\n\n\n\n\n\n\n\n\n\n\n\nPrimer on Likelihood\n\n\n\n\n\n\nlikelihood\n\n\nfisher information\n\n\nnewton raphson\n\n\n\nApplied to parameter estimation\n\n\n\n\n\nDec 28, 2024\n\n\nPrateek\n\n\n\n\n\n\n\n\n\n\n\n\nUser behaviour analysis\n\n\n\n\n\n\nmarkov chain\n\n\ntimeseries\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\nPrateek\n\n\n\n\n\n\n\n\n\n\n\n\nAre blockbusters getting (reely) longer ?\n\n\n\n\n\n\nhypothesis testing\n\n\nbootstrap\n\n\nregression\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nPrateek\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/teen_gamble/index.html#objective",
    "href": "posts/teen_gamble/index.html#objective",
    "title": "Teen gambling analysis",
    "section": "Objective",
    "text": "Objective\nThe objective of this analysis is to understand the factors that influence teenage gambling expenditure. We will explore the relationships between gambling expenditure and various predictors."
  },
  {
    "objectID": "posts/teen_gamble/index.html#data",
    "href": "posts/teen_gamble/index.html#data",
    "title": "Teen gambling analysis",
    "section": "Data",
    "text": "Data\nThe analysis is based on data from Ide-Smith and Lea (1988) published in the Journal of Gambling Behavior. It comes from a survey that was conducted to study teenage gambling in Britain. \nThe key variables are:\n\nsex\nstatus : Socioeconomic status score based on parents’ occupation\nincome : Weekly income in pounds\nverbal : verbal score in words out of 12 correctly defined\ngamble : expenditure on gambling in pounds per year\n\n\n\n\n\n\nsex\nstatus\nincome\nverbal\ngamble\n\n\n\n\nfemale\n51\n2.00\n8\n0.0\n\n\nfemale\n28\n2.50\n8\n0.0\n\n\nfemale\n37\n2.00\n6\n0.0\n\n\nfemale\n28\n7.00\n4\n7.3\n\n\nfemale\n65\n2.00\n8\n19.6\n\n\nfemale\n61\n3.47\n6\n0.1\n\n\n\n\n\nA cursory look at the data suggests that females likely spend less on gambling compared to males. Additionally that most females do not spend more than approx. 20£ per year on gambling.\n\n\n\n\n\n\n\n\n\nThere also seems positive association between income and amount spent on gambling (at least for males). This is not very surprising as one would expect that people with higher income would be able to spend more on gambling.  From what this plot indicates, it may be worth evaluating if there is a differential effect of income based on sex."
  },
  {
    "objectID": "posts/teen_gamble/index.html#analysis",
    "href": "posts/teen_gamble/index.html#analysis",
    "title": "Teen gambling analysis",
    "section": "Analysis",
    "text": "Analysis\nOur variable of interest gamble is strictly positive and continuous with a long tail. We consider modelling log of gamble value. This alters the interpretation of model mechanics but it remains sensible, e.g. males spend x% more than females or 1£ increase in income leads to y% increase in expenditure in gambling.  Side note: This is different from using GLM with Gaussian family and log link. Because in original scale, variance is non constant and multiplicative effects are expected on original scale of response (as opposed to expected value of response). \nTo be able to take logarithms of cases where 0£ are spent on gambling, 1£ is added to gambling spent for all cases. This value is chosen to ensure 0 for cases where 0£ are spent and values close to 0 for small spends, additionally this does not introduce un necessary -ve outlier values resulting in better fit. \n\n\n\nCall:\nlm(formula = log(gamble + 1) ~ sex + income, data = teengamb)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.27059 -0.77305  0.08595  0.84018  2.32870 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.31035    0.33082   0.938 0.353298    \nsexmale      1.28432    0.34421   3.731 0.000543 ***\nincome       0.19312    0.04808   4.017 0.000227 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.15 on 44 degrees of freedom\nMultiple R-squared:  0.4357,    Adjusted R-squared:   0.41 \nF-statistic: 16.98 on 2 and 44 DF,  p-value: 3.416e-06\n\n\n\n\n\n\n\n\n\nWe find that there is no obvious problem with the errors. Additionally, the fit is decent and the coefficients for both Sex (Male) and Income are significant. \nJensen’s inequality dictates that simply taking exponents of predictions will under-predict the expected value of response variable f(E(X)) &lt;= E(f(X)) , a custom predict method is defined which also corrects for the 1£ offset used in model."
  },
  {
    "objectID": "posts/teen_gamble/index.html",
    "href": "posts/teen_gamble/index.html",
    "title": "Teen gambling analysis",
    "section": "",
    "text": "- The project is out-of-sync -- use `renv::status()` for details."
  },
  {
    "objectID": "posts/teen_gamble/index.html#introduction",
    "href": "posts/teen_gamble/index.html#introduction",
    "title": "Teen gambling analysis",
    "section": "",
    "text": "- The project is out-of-sync -- use `renv::status()` for details."
  }
]