[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Prateek",
    "section": "",
    "text": "I am a Lead Data Scientist at Bricklane Technologies. When not dabbling in data analysis, I enjoy spending time reading or running."
  },
  {
    "objectID": "posts/movies_getting_longer/index.html",
    "href": "posts/movies_getting_longer/index.html",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "",
    "text": "I recently came across an article posing a question:\n\nAre blockbusters getting (reely) longer?\n\nFueled by the buzz around Christopher Nolan’s Oppenheimer being his longest movie (just over 3 hours), I decided to explore this question using publicly available data.\nAs an exercise in inference this question can be answered with some analysis. I fetched top grossing movies in each year from boxoffice mojo and movie details from imdb and combined them to create the following dataset.\ndata\nThe code to download data is also in this very same .qmd file"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#introduction",
    "href": "posts/movies_getting_longer/index.html#introduction",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "",
    "text": "I recently came across an article posing a question:\n\nAre blockbusters getting (reely) longer?\n\nFueled by the buzz around Christopher Nolan’s Oppenheimer being his longest movie (just over 3 hours), I decided to explore this question using publicly available data.\nAs an exercise in inference this question can be answered with some analysis. I fetched top grossing movies in each year from boxoffice mojo and movie details from imdb and combined them to create the following dataset.\ndata\nThe code to download data is also in this very same .qmd file"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#data",
    "href": "posts/movies_getting_longer/index.html#data",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "Data",
    "text": "Data\n\nyearly_top_movies = read_delim('movies_dataset.csv',\n show_col_types = FALSE)\nhead(yearly_top_movies)\n\n# A tibble: 6 × 3\n  release_year runtime_mins title                       \n         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                       \n1         1990          103 Home Alone                  \n2         1990          127 Ghost                       \n3         1990          181 Dances with Wolves          \n4         1990          119 Pretty Woman                \n5         1990           93 Teenage Mutant Ninja Turtles\n6         1990          135 The Hunt for Red October    \n\n\n\n# Scatterplot of movie runtime over time\nggplot(yearly_top_movies, aes(x = release_year, y = runtime_mins)) +\n  geom_point() + # Scatter points for movie runtimes\n  geom_text(data = annotations, aes(label = title), color = \"red\", nudge_y = 5) + # Annotate specific movies\n  geom_line(data = mean_df, aes(y = runtime_mins), color = \"brown\") + # Line for yearly mean runtimes\n  ggtitle(\"Top Grossing Movies per Year\") +\n  xlab(\"Release Year\") +\n  ylab(\"Runtime (mins)\")"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#analysis",
    "href": "posts/movies_getting_longer/index.html#analysis",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "Analysis",
    "text": "Analysis\n\nComparing recent vs old movies\nTo simplify the analysis we can consider more recent releases (2013-2023) and compare them to releases from much older timeframe (1990-2000).  The runtime distributions for movies in these 2 categories is different although not significantly (visually).\n\ntestset &lt;- yearly_top_movies %&gt;%\n    mutate(\n    label = case_when(\n      release_year &gt;= 2013 ~ \"recent\",\n      release_year &lt;= 2000 ~ \"old\",\n      TRUE ~ \"other\"\n    )) %&gt;% filter(label != \"other\")\n\nggplot(testset, aes(x=runtime_mins, fill=label)) + geom_density(alpha=0.3)\n\n\n\n\n\n\n\n\nNow we can compare runtimes between two periods. It would have been a straightforward exercise in testing the hypothesis of difference of means using a t-test had the distributions of runtimes been more normally distributed. \nAdditionally, the cutoff chosen for old (&lt;= 2000) and recent (&gt;=2013) are arbitrary. We would ideally want a more comprehensive statement about movie runtimes increasing over the years. \nNeverthless, as a preliminary (and simple) analysis we can use bootstrap to compare means of runtimes in the above groups.\n\nlibrary(boot)\n\n# Define a function to calculate the difference of means\ndiff_mean &lt;- function(data, indices) {\n  # Resample the data\n  resampled_data &lt;- data[indices, ]\n\n  # Calculate means for each group\n  mean_group1 &lt;- mean(resampled_data$runtime_mins[resampled_data$label == \"old\"])\n  mean_group2 &lt;- mean(resampled_data$runtime_mins[resampled_data$label == \"recent\"])\n\n  # Return the difference of means\n  return(mean_group2 - mean_group1)\n}\n\nset.seed(123)\nbootstrap_results &lt;- boot(data = testset, statistic = diff_mean, R = 1000)\n\n# Print bootstrap results\nprint(bootstrap_results)\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = testset, statistic = diff_mean, R = 1000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 9.697248 0.06394496    3.174053\n\n# Plot bootstrap distribution\nplot(bootstrap_results)\n\n\n\n\n\n\n\n# Calculate confidence interval for the difference of means\nci &lt;- boot.ci(bootstrap_results, type = \"perc\")\nprint(ci)\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap_results, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 3.502, 15.736 )  \nCalculations and Intervals on Original Scale\n\n\nIf we bootstrap the mean of movie times and take the difference between means of recent release years from old, there is evidence that blockbuster movies are getting longer in recent years, and the difference can be up to 10 min on average and can be expected to be between 4 min and 16 min. \nSo on your next visit to the theatre make sure to get some extra popcorn !!!\n\n\nEstimating effect of time\nIn the want of better precision in identifying the effect of time on movie runtimes, we can formulate a regression problem and estimate the effect of year.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe plot above partly confirms our intuition about the increasing trend of runtime.\n\nlmod &lt;- lm(runtime_mins ~ release_year, yearly_top_movies)\nsummary(lmod)\n\n\nCall:\nlm(formula = runtime_mins ~ release_year, data = yearly_top_movies)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.369 -19.617   0.488  16.422  77.834 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -636.1435   271.4520  -2.343  0.01969 * \nrelease_year    0.3791     0.1353   2.802  0.00537 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.42 on 335 degrees of freedom\nMultiple R-squared:  0.0229,    Adjusted R-squared:  0.01999 \nF-statistic: 7.852 on 1 and 335 DF,  p-value: 0.005372\n\n\nThe regression above supports the hypothesis that release_year has positively contributed to movie runtimes at approximately 0.38 min/year.  Considering our previous finding on average between 1995 and 2018 the runtime increased by approx. 10 min. From the regression estimate the increase is 0.3791 * (2018-1995) = 8.7 min. \nWe can certainly run some diagnostics to be sure of our model.\n\n\n\n\n\n\n\n\n\nThe residuals appear to have no pattern with fitted values and also have fairly constant variance. There are no significantly high leverage points or outliers.  However, the errors are not quite normally distributed.  This can be ignored by relying on large sample size and the fact that other diagnostics are fine.  But, we shall check if just in case any transformation of runtime_mins may help.\n\nsuppressPackageStartupMessages(library(MASS))\nboxcox(lmod, lambda=seq(-1,1,by=0.1))\n\n\n\n\n\n\n\n\nThe Box-Cox transformation check above hints that there may be some benefit if we used log-transformation (since lambda approximately 0). This thread does not materialise into anything meaningful as upon transformation, the diagnostics do not change materially and the effect size of release_year is practically the same (0.32% per year, 0.38 min/year on base of 119 min). So we will not use any transformation in our model.\nThere may also be some degree of autocorrelation among residuals due to the fact that we’re working with timeseries data. We can test for this autocorrelation and handle it if necessary.\n\n\n[1] \"Auto-correlation among residuals: \" \"0.01\"                              \n\n\n\n    Durbin-Watson test\n\ndata:  runtime_mins ~ release_year\nDW = 1.982, p-value = 0.4126\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nThere doesn’t appear to be significant autocorrelation and other model diagnotics appear to be fine."
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#conclusion",
    "href": "posts/movies_getting_longer/index.html#conclusion",
    "title": "Are blockbusters getting (reely) longer ?",
    "section": "Conclusion",
    "text": "Conclusion\nThere has been a rather steady increase of 0.38 min/year (on average) in movie runtimes as per the dataset we have used. In effect expect to be seated for about 10 more minutes in your favourite blockbuster compared to when you went to movies as a kid."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html",
    "href": "posts/user_behaviour_analysis/index.html",
    "title": "User behaviour analysis",
    "section": "",
    "text": "Duolingo have been making waves with their customer success stories and strong (and growing) user base. They seem to be taking their metrics seriously and have done a great job explaining their approach to understanding user behaviour in this blog post.  In this blog, I wanted to apply their methodology and use it for predicting future customer behaviours."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#introduction",
    "href": "posts/user_behaviour_analysis/index.html#introduction",
    "title": "User behaviour analysis",
    "section": "",
    "text": "Duolingo have been making waves with their customer success stories and strong (and growing) user base. They seem to be taking their metrics seriously and have done a great job explaining their approach to understanding user behaviour in this blog post.  In this blog, I wanted to apply their methodology and use it for predicting future customer behaviours."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#methodology",
    "href": "posts/user_behaviour_analysis/index.html#methodology",
    "title": "User behaviour analysis",
    "section": "Methodology",
    "text": "Methodology\nFirst we shall model the process as a Markov chain to see how well the approach predicts future user counts.  Subsequently, to validate MC approach and rethink the problem setup in want of simplification, we shall look to model the time-series as a regression problem. We shall do this respecting the mechanics of transition and see how far this can get us in terms of analysing and controlling the process.\nAt day d (d = 1, 2, … ) of a user’s lifetime, the user can be in one of the following 7 (mutually-exclusive) states: \n\nnew\ncurrent\nreactivated\nresurrected\nat_risk_wau (weekly active user: active in last 7 days)\nat_risk_mau (monthly active user: active in last 30 days but not last 7 days)\ndormant\n\nThese states are defined according to indicators of whether a user was active today, in the last 7 days, or in the last 30 days. A brief overview of how these states are related is below:\n\n\n\nUser states and transitions\n\n\nIn keeping with the definition chart above, the next step is to consider user behaviour as a Markov chain.  Let M be a transition matrix associated with this Markov process: m(i, j) = P(s_j | s_i) are the probabilities that a user moves to state s_j right after being at state s_i. The matrix M is learned from the historical data.\nAssuming that user behavior is stationary (independent of time), the matrix M fully describes the states of all users in the future.\nSuppose that the vector u_0 of length 7 contains the counts of users in certain states on a given day (say day 0). According to the Markov model, on the next day (day 1), we expect to have the following number of users (u_1) in respective states:\n\n\n\nState transition counts estimation\n\n\nApplying this multiplication recursively, we can derive the number of users in any states on any arbitrary day t &gt; 0 in the future (call this vector u_t).\nNow, having u_t calculated, we can determine DAU, WAU and MAU values on day t:\n\nDAU_t = #New_t + #Current_t + #Reactivated_t + #Resurrected_t\nWAU_t = DAU_t + #AtRiskWau_t\nMAU_t = DAU_t + #AtRiskWau_t + #AtRiskMau_t\n\nFinally, here’s the algorithm outline:\n\nFor each prediction day t = 1, …, T, calculate the expected number of new users #New_1, …, #New_T.\nFor each lifetime day of each user, assign one of the 7 states.\nCalculate the transition matrix M from the historical data.\nCalculate initial state counts u_0 corresponding to day t=0.\nRecursively calculate u_{t+1} = M^t * u_0.\nCalculate DAU, WAU, and MAU for each prediction day t = 1, …, T."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#implementation",
    "href": "posts/user_behaviour_analysis/index.html#implementation",
    "title": "User behaviour analysis",
    "section": "Implementation",
    "text": "Implementation\n\nData\nWe use a simulated dataset based on historical data of a SaaS app.  The data is available here and contains three columns: user_id, date, and registration_date.\nEach record indicates a day when a user was active. \n\ndau_data &lt;- read_csv(\"dau_data.csv\", show_col_types = FALSE)\nhead(dau_data)\n\n# A tibble: 6 × 3\n  user_id                              date       registration_date\n  &lt;chr&gt;                                &lt;date&gt;     &lt;date&gt;           \n1 d8c465ab-e9fd-5edd-9e4e-c77094700cb5 2020-10-01 2020-08-25       \n2 269b7f13-a509-5174-85cb-95a8f7b932e8 2020-10-01 2020-08-25       \n3 bfeac474-5b66-566f-8654-262bb79c873e 2020-10-01 2020-05-31       \n4 d32fcac5-122c-5463-8aea-01b39b9ad0bb 2020-10-01 2020-09-30       \n5 c1ece677-e643-5bb3-8701-f1c59a0bf4cd 2020-10-01 2020-09-05       \n6 ba6750c9-9b60-58dc-8d40-504e7375249e 2020-10-01 2020-09-29       \n\n\n\n\nTotal users: 51480\n\n\nDate range: 2020-10-01 to 2023-10-31\n\n\nThis is how DAU timeseries looks\n\n\n\n\n\n\n\n\n\n\n\nFuture new user count model\nTo even begin thinking about transitions on a future date, we first need to acknowledge the fact that new user counts will be a significant missing piece of information in the analysis.  To get around this limitation, we shall first build a new user count prediction model that need not be very accurate (since in a realistic setting new user counts can also be a parameter that can be changed).\nThe information on new users on any given date is inherent in registration_date in the data.\n\nnew_users &lt;- dau_data %&gt;%\n                filter(date==registration_date) %&gt;%\n                group_by(date) %&gt;%\n                summarise(new_user_count=n()) %&gt;%\n                mutate(\n                    year = year(date),\n                    week_of_year = isoweek(date),  # ISO 8601 week (Monday starts the week)\n                    month = as.factor(month(date, label = TRUE, abbr = TRUE)),  # Month as abbreviated name\n                    day_of_week = wday(date)\n\n                    )\n\nlmod &lt;- glm.nb(new_user_count ~ year * bs(week_of_year,4) + month + day_of_week , data=new_users)\n\n# summary(lmod)\n\nAfter some experimentation, the model setup to predict future counts of new users is as above.  The counts are assumed Poisson but with a higher variance than normal due to spikes in the data and hence a negative binomial model has been used.  It was imperative that we used a robust version of GLM to be able to get relatively constant variance of errors, that were being caused due to outliers in user count towards beginning of every year.  The model appears to be doing a satisfactory job of predicting counts of new users and we consider this acceptable for further analysis.\n\n\n\n\n\n\n\n\n\n\n\nAssigning states"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Room",
    "section": "",
    "text": "User behaviour analysis\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\nmarkov-chain\n\n\n\n\n\n\n\n\n\nDec 18, 2024\n\n\nPrateek\n\n\n\n\n\n\n\n\n\n\n\n\nAre blockbusters getting (reely) longer ?\n\n\n\n\n\n\nanalysis\n\n\ncode\n\n\n\n\n\n\n\n\n\nJul 21, 2023\n\n\nPrateek\n\n\n\n\n\n\nNo matching items"
  }
]