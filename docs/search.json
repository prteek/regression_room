[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Prateek",
    "section": "",
    "text": "I am a Lead Data Scientist at Bricklane Technologies. When not dabbling in data analysis, I enjoy spending time reading or running."
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/index.html",
    "href": "posts/occupation_impact_on_rentals/index.html",
    "title": "rentership ~ occupation, analysis",
    "section": "",
    "text": "Private renting has expanded across the UK, yet the propensity to rent varies widely across occupations and life stages. This study quantifies how occupation and age relate to private rentership using 2021 ONS Census cross‑tabulations at the MSOA (neighbourhood‑scale) level. Housing tenure for household reference persons is linked to occupational categories and augmented with local authority identifiers to capture geographic context.\nThe analysis addresses three questions: \n\nDo higher‑skilled occupations rent less than lower‑skilled groups after accounting for age composition?\nHow does rentership vary across life stages (age) ?\nTo what extent do local authorities and neighbourhoods moderate these relationships?\n\nThe study is descriptive and subject to measurement constraints: age composition is observed for all workers rather than HRPs, and the focus is on full‑time employment. Despite these limitations, the findings provide a transparent baseline on how occupational structure and local context shape rental demand and motivate future work with richer microdata and dynamic market indicators."
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/index.html#data",
    "href": "posts/occupation_impact_on_rentals/index.html#data",
    "title": "rentership ~ occupation, analysis",
    "section": "Data",
    "text": "Data\nWe use 2021 ONS Census data (via the Nomis API) that cross‑tabulates housing tenure and occupation at the MSOA level (“RM140: Tenure by Occupation - Household Reference Persons” 2023).  MSOAs (Middle Layer Super Output Areas) are neighborhood‑sized geographies. Counts refer to the household reference person (HRP) the person who represents the household, so each household is linked to a single occupation.  These are different from the counts of all workers (used in occupation‑by‑age data introduced later in the study), because a household can contain more than one person in full‑time work but only one HRP. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2021 super output area - middle layer\nTotal\n1. Managers, directors and senior officials\n2. Professional occupations\n3. Associate professional and technical occupations\n4. Administrative and secretarial occupations\n5. Skilled trades occupations\n6. Caring, leisure and other service occupations\n7. Sales and customer service occupations\n8. Process, plant and machine operatives\n9. Elementary occupations\nTenure of household\n\n\n\n\nE02001721 : Newcastle upon Tyne 014\n549\n59\n103\n65\n46\n89\n52\n36\n47\n52\nOwns outright\n\n\nE02000092 : Bexley 028\n254\n29\n43\n23\n48\n35\n20\n20\n16\n20\nOwns outright\n\n\nE02003977 : Barrow-in-Furness 001\n524\n56\n98\n50\n38\n89\n46\n41\n47\n59\nOwns outright\n\n\nE02004544 : Harlow 001\n460\n74\n75\n62\n40\n64\n37\n25\n45\n38\nPrivate rented or lives rent free\n\n\nE02000905 : Waltham Forest 011\n1262\n252\n452\n278\n72\n55\n44\n37\n41\n31\nOwns with a mortgage or loan or shared ownership\n\n\nE02004977 : Watford 010\n1221\n71\n237\n116\n67\n173\n139\n73\n130\n215\nPrivate rented or lives rent free\n\n\n\n\n\n\nAdditional information\nLocal authority for each MSOA in original dataset is added, to account for rental patterns that may be present at local area levels. Being in the same Local authority may affect neighbouring MSOAs introducing correlations in their data. \nIt may be the case that age has a confounding effect on Rentership via Occupation. Professional and Managers may likely be older than for e.g. Associates and this may mean that they were able to purchase a house earlier in their careers/life and now represent a smaller proportion of rental cohort.  Adjusting for age gives an apples-to-apples comparison of rentership preference among people pursuing different occupations but are in boradly same phases of their life.  The adjustment can be made (ideally) by including the counts of full-time employed HRPs in an MSOA engaged in a particular profession.\nWe have cross tabulated data (“RM102: Occupation by Age - Residents in Employment” 2023) for counts of usual residents aged 16 years and over in employment the week before the census per occupation, falling in one of the following age bands: \n\naged 15 years and under\naged 16 to 24 years\naged 25 to 34 years\naged 35 to 49 years\naged 50 to 64 years\naged 65 years and over\n\nNote:  Using this data gives an inexact adjustment since this data is available for all residents aged 16 years and over in employment rather than just HRP (which is the unit of measurement in tenure data). This means that an HRP could be a singular older person, whereas the counts in age dataset include other members of the household which are possibly younger, creating a slight disconnect between the 2 datasets.  It is neverthless instructive to use this data as the downsides due to excluding this information outweigh downsided due to (incorrectly) including it.\nSince, the interest lies in proportion of rentership for each occupation, per occupation totals for each MSOA are added to the dataset, setting us up for a Binomial counts of Success-Failures type of model. \n\n\nFilter\nCurrent data only includes measurements from household reference persons (HRP) in full time employment a week before census.  London is also filtered out as it is a very different rental market compared to the rest of UK and may skew results when mixed with other regions. \nNote: \n\nONS rental tenure data includes a large proportion (~34%) of people not under full time employment(“Annex Table 1.5: Employment Status by Tenure,2021-22” 2023) (filtered out in current analysis)\n\npart-time work (~11% includes furloughed)\nretired (~7%)\nunemployed (~4%)\nfull-time education (~4%)\nother inactive (~8%)\n\nThe private rented sector houses the highest proportion of non-UK nationals (74% of HRPs in the private rented sector are from the UK, compared to 92% of social renters and 96% of owner occupiers) (“English Housing Survey 2021 to 2022: Private Rented Sector” 2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmsoa\ntenure\noccupation\ncounts_of_hrp\noccupation_total\naged 15 years and under\naged 16 to 24 years\naged 25 to 34 years\naged 35 to 49 years\naged 50 to 64 years\naged 65 years and over\nall_ages_total\nregion\nlad_name\n\n\n\n\ne02005594\nprivate rented or lives rent free\n9. elementary occupations\n120\n290\n0\n142\n124\n132\n128\n12\n538\ne12000006\nnorwich\n\n\ne02001176\nprivate rented or lives rent free\n4. administrative and secretarial occupations\n76\n258\n0\n58\n135\n150\n166\n20\n529\ne12000002\nsalford\n\n\ne02005706\nprivate rented or lives rent free\n1. managers, directors and senior officials\n44\n234\n0\n8\n47\n124\n138\n31\n348\ne12000001\nnorthumberland\n\n\ne02003892\nprivate rented or lives rent free\n4. administrative and secretarial occupations\n16\n110\n0\n14\n38\n61\n107\n22\n242\ne12000009\ncornwall\n\n\ne02002029\nprivate rented or lives rent free\n3. associate professional and technical occupations\n29\n228\n0\n29\n93\n174\n115\n13\n424\ne12000005\nbromsgrove\n\n\ne02002145\nprivate rented or lives rent free\n5. skilled trades occupations\n49\n282\n0\n32\n110\n143\n115\n13\n413\ne12000005\nsandwell\n\n\n\n\n\n\n\nSubsample\nSince, the dataset is very well compiled but is large, we can work with reduced dataset and infer large scale effects from it. This reduction will allow speeding up the analysis and experimentation. \nWe can see that majority of Local authorities have 10-15 MSOAs. To get a balanced representation, we chose to sample 10 MSOA from each Local authority. This will likely also ensure robust estimation of any random effects.\n\n\n\n\n\n\n\n\n\nThe subsampling process does the following: \n\nSample all local authorities from 8 UK wide regions (e.g. West midlands, East of England etc.) but excluding London region (since market behaviour may not generalise to other regions and create issues when data is mixed with others, due to very high counts)\nWithin each of the local authorities sample 10 MSOAs at random\nGet all the data in the sampled MSOAs\n\nThe above procedure is deliberate in that it results in balanced data among Local authorities to avoid issues with inference.  It goes without saying that this step means that Local Authorities and MSOAs need to be treated as random effects down the line (with appropriate heirarchy).  The procedure ensures sufficient counts of Local Authorities and MSOAs to be able to reliably capture these random effects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrows\ndistinct_tenure\ndistinct_occupation\nmax_hrp_counts\nmin_hrp_counts\nmedian_hrp_counts\ndistinct_lad\ndistinct_msoa\n\n\n\n\n20124\n1\n9\n788\n0\n40\n234\n2236\n\n\n\n\n\n\n\nEDA\nAt the level of MSOA the proportion of rentership can appear quite chaotic and noisy.\n\n\n\n\n\n\n\n\n\nAggregated data shows clear differences in renting by occupation. Highly skilled and professional workers rent less, likely because they can more easily afford to buy.\n\n\n\n\n\n\n\n\n\nThe observation holds and patterns become more clear when the data is aggregated at Local authority level. This means that a heirarchical consideration of geographies could be a sensible inclusion in the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a slightly long tail of working population per msoa. \nLooking at the msoa where the greatest proportion of working population is renting, a few key dense local authority areas are highlighted\n\n\n\n\n\n\n\n\n\n\n\nmsoa\nlad_name\ncounts_of_hrp\naggregate_proportion_renting\n\n\n\n\ne02005029\ndartford\n4969\n0.262\n\n\ne02004650\ntewkesbury\n4484\n0.224\n\n\ne02004681\nbasingstoke and deane\n4298\n0.185\n\n\ne02005683\nwest northamptonshire\n4266\n0.168\n\n\ne02005614\nnorth northamptonshire\n4208\n0.221\n\n\ne02004595\nuttlesford\n4175\n0.156\n\n\ne02003643\ncentral bedfordshire\n4169\n0.166\n\n\ne02005455\nnorth kesteven\n4147\n0.182\n\n\ne02005924\ncherwell\n4146\n0.485\n\n\ne02006270\nmid suffolk\n4093\n0.266\n\n\n\n\n\nThe age distribution of working population varies significantly across occupations.  It is interesting to note that some occupations are over represented among older age groups (e.g. Managers and Professionals), whereas others are more evenly spread (e.g. Elementary operations). \n\n\n\n\n\n\n\n\n\nSince, age data has a long tailed distribution, log scaling is used to better scale it and also make its interpretation in the model more intuitive."
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/index.html#dataset",
    "href": "posts/occupation_impact_on_rentals/index.html#dataset",
    "title": "rentership ~ occupation, analysis",
    "section": "Dataset",
    "text": "Dataset\n\nAdditional information\nLocal authority for each MSOA in original dataset is added, to account for rental patterns that may be present at local area levels. Being in the same Local authority may affect neighbouring MSOAs introducing correlations in their data. \nIt can be possible that age has a confounding effect on Rentership via Occupation. Majority of Professional and Managers may likely be older than associates and this may mean that they were able to purchase a house earlier in their careers/life and now represent a smaller proportion of rental cohort.  Adjusting for age gives an apples-to-apples comparison of rentership preference among people pursuing different occupations but are in boradly same phases of their life.  The adjustment is made by including the proportion of full-time employed HRPs in an MSOA engaged in a particular profession falling in one of the following age bands: \n\naged 15 years and under\naged 16 to 24 years\naged 25 to 34 years\naged 35 to 49 years\naged 50 to 64 years\naged 65 years and over\n\nNote:  This adjustment is inexact since the data is available for all people in full time occupation rather than just HRP (which is the unit of measurement in tenure data). This means that likely an HRP would be an older person, the counts in age dataset include other members of the household which are possibly younger, creating a slight disconnect between the 2 datasets.  It is neverthless instructive to use this information as the downsides due to excluding this information outweigh downsided due to (incorrectly) including it.\nSince, the interest lies in proportion of rentership for each occupation, per occupation totals for each MSOA are added to the dataset.\n\n\nFilter\nCurrent data only includes measurements from household reference persons (HRP) in full time employment a week before census.  It would be sensible to drop data if any occupation icluded positive entries in aged 15 years and under column.\nNote: \n\nONS rental tenure data includes a large proportion (~34%) of people not under full time employment(“Annex Table 1.5: Employment Status by Tenure,2021-22” 2023) (filtered out in current analysis)\n\npart-time work (~11% includes furloughed)\nretired (~7%)\nunemployed (~4%)\nfull-time education (~4%)\nother inactive (~8%)\n\nThe private rented sector houses the highest proportion of non-UK nationals (74% of HRPs in the private rented sector are from the UK, compared to 92% of social renters and 96% of owner occupiers) (“English Housing Survey 2021 to 2022: Private Rented Sector” 2023)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmsoa\ntenure\noccupation\ncounts_of_hrp\noccupation_total\naged 15 years and under\naged 16 to 24 years\naged 25 to 34 years\naged 35 to 49 years\naged 50 to 64 years\naged 65 years and over\nall_ages_total\nregion\nlad_name\n\n\n\n\ne02005011\nprivate rented or lives rent free\n9. elementary occupations\n21\n97\n0\n0.2592593\n0.1563786\n0.2057613\n0.3251029\n0.0534979\n243\ne12000008\ncanterbury\n\n\ne02005139\nprivate rented or lives rent free\n3. associate professional and technical occupations\n50\n248\n0\n0.0911162\n0.1708428\n0.3394077\n0.3416856\n0.0569476\n439\ne12000008\nthanet\n\n\ne02002149\nprivate rented or lives rent free\n1. managers, directors and senior officials\n37\n269\n0\n0.0123762\n0.2227723\n0.3688119\n0.3391089\n0.0569307\n404\ne12000005\nsouth staffordshire\n\n\ne02001798\nprivate rented or lives rent free\n3. associate professional and technical occupations\n17\n157\n0\n0.1006711\n0.2651007\n0.3422819\n0.2684564\n0.0234899\n298\ne12000001\nsunderland\n\n\ne02002824\nprivate rented or lives rent free\n6. caring, leisure and other service occupations\n57\n285\n0\n0.0848375\n0.2382671\n0.3808664\n0.2725632\n0.0234657\n554\ne12000004\nderby\n\n\ne02002935\nprivate rented or lives rent free\n4. administrative and secretarial occupations\n40\n144\n0\n0.0731707\n0.2404181\n0.2508711\n0.3937282\n0.0418118\n287\ne12000005\ntelford and wrekin\n\n\n\n\n\n\n\nSubsample\nSince, the dataset is very well compiled but is large, we can work with reduced dataset and infer large scale effects from it. This reduction will allow speeding up the analysis and experimentation.  The subsampling process does the following: \n\nRandomly sample 10 local authorities from 8 UK wide regions (e.g. West midlands, East of England etc.) excluding London region (since market behaviour may not generalise to other regions and create issues when data is mixed with others, due to very high counts)\nWithin each of the sampled local authorities sample 50 MSOAs at random\nGet all the data in the sampled MSOAs\n\nThe above procedure is deliberate in that it results in balanced data among Local authorities to avoid issues with inference.  It goes without saying that this step means that Local Authorities and MSOAs need to be treated as random effects down the line (with appropriate heirarchy).  The procedure ensures sufficient counts of Local Authorities and MSOAs to be able to reliably capture these random effects.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrows\ndistinct_tenure\ndistinct_occupation\nmax_counts\nmin_counts\nmedian_counts\ndistinct_lad\ndistinct_msoa\n\n\n\n\n18495\n1\n9\n882\n2\n40\n80\n2055"
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/index.html#eda",
    "href": "posts/occupation_impact_on_rentals/index.html#eda",
    "title": "rentership ~ occupation, analysis",
    "section": "EDA",
    "text": "EDA\nAt the level of MSOA the proportion of rentership can appear quite chaotic and noisy.\n\n\n\n\n\n\n\n\n\nAggregated data shows clear differences in renting by occupation. Highly skilled and professional workers rent less, likely because they can more easily afford to buy.\n\n\n\n\n\n\n\n\n\nThe observation holds and patterns become more clear when the data is aggregated at Local authority level. This means that a heirarchical consideration of geographies could be a sensible inclusion in the model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere is a slightly long tail of working population per msoa. \nLooking at the msoa where the greatest proportion of working population is renting, a few key dense local authority areas are highlighted\n\n\n\n\n\n\n\n\n\n\n\nlad_name\nmsoa\naggregate_proportion_renting\ntotal_working_population\n\n\n\n\nbradford\ne02006948\n0.728\n1660\n\n\nleeds\ne02002373\n0.719\n2022\n\n\nleeds\ne02006875\n0.712\n2797\n\n\nleeds\ne02002392\n0.711\n1508\n\n\nleicester\ne02002849\n0.694\n3566\n\n\neast riding of yorkshire\ne02002666\n0.677\n2259\n\n\nsouthampton\ne02003565\n0.650\n1931\n\n\nleicester\ne02002842\n0.641\n2504\n\n\nsouthampton\ne02003571\n0.627\n2236\n\n\npeterborough\ne02003250\n0.623\n3272"
  },
  {
    "objectID": "posts/likelihood_primer/index.html",
    "href": "posts/likelihood_primer/index.html",
    "title": "Likelihood",
    "section": "",
    "text": "Likelihood is a fundamental concept that plays a pivotal role in parameter estimation and model fitting.  Whether one is estimating the parameters of a simple distribution or building complex statistical models, likelihood functions provide a way to quantify how likely a set of observed data is, given specific model parameters.  In this post, we’ll break down the concept of likelihood, explore how it relates to probability, and demonstrate its application in parameter estimation using methods of Maximum Likelihood Estimation (MLE) and Newton Raphson.  This primer will help bring clarity to working with Likelihoods for parameter estimation.  The concepts discussed are deeply inspired by (Faraway 2016) and (George Casella 2002). For a more comprehensive understanding of the subject, I highly recommend reading the full texts."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#introduction",
    "href": "posts/likelihood_primer/index.html#introduction",
    "title": "Likelihood",
    "section": "",
    "text": "Likelihood is a fundamental concept that plays a pivotal role in parameter estimation and model fitting.  Whether one is estimating the parameters of a simple distribution or building complex statistical models, likelihood functions provide a way to quantify how likely a set of observed data is, given specific model parameters.  In this post, we’ll break down the concept of likelihood, explore how it relates to probability, and demonstrate its application in parameter estimation using methods of Maximum Likelihood Estimation (MLE) and Newton Raphson.  This primer will help bring clarity to working with Likelihoods for parameter estimation.  The concepts discussed are deeply inspired by (Faraway 2016) and (George Casella 2002). For a more comprehensive understanding of the subject, I highly recommend reading the full texts."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#likelihood",
    "href": "posts/likelihood_primer/index.html#likelihood",
    "title": "Likelihood",
    "section": "Likelihood",
    "text": "Likelihood\n\nDiscrete RV case\nIf we consider there to be a set of independent discrete random variables \\(Y_1,...., Y_n\\) with probability mass function \\(f(y|\\theta)\\) where \\(\\theta\\) is the parameter.  Let \\(\\mathbf{y} = (y_1,....,y_n)^T\\) be the observed values, then the likelihood function for the distribution is:\n\\[\nP(\\mathbf{Y}=\\mathbf{y}) = \\prod_{i=1}^{n} f(y_i|\\theta) = L(\\theta|y)\n\\]\nAs can be seen likelihood is about considering parameter \\(\\theta\\) as random and data \\(\\mathbf{y}\\) as fixed (hence the notation \\(L(\\theta|y)\\)). This is as opposed to a joint density function where \\(\\theta\\) are fixed and \\(y\\) are random.  Effectively though likelihood is still the probability of observed data \\(\\mathbf{y}\\) for a specified value of the parameter(s) i.e. \\(p(\\mathbf{y}|\\theta)\\) but the key distinction is that likelihood changes as \\(\\theta\\) changes.\n\n\nContinuous RV case\nWhen the random variables are continuous (with probability density \\(f(y|\\theta)\\)), in practice one can measure data with limited precision. When a value \\(y_i\\) is observed it effectively indicates an observation in the range \\([y_i^l, y_i^u]\\), where \\(y_i^l\\) and \\(y_i^u\\) are lower and upper bounds of precision limit.  Using which we can reason that probability of observed value is a concept defined over an interval:\n\\[\nP(Y_i=y_i) = P(y_i^l&lt;=y_i&lt;=y_i^u) = \\int_{y_i^l}^{y_i^u} f(u|\\theta) \\, du \\approx f(y_i|\\theta) \\delta_i\n\\]\nsuch that \\(\\delta_i = y_i^u-y_i^l\\), which does not depend on \\(\\theta\\) and is related to the precision of measurement.  And, the likelihood becomes:\n\\[\nL(\\theta|y) \\approx \\prod_{i=1}^{n} f(y_i|\\theta) \\prod_{i=1}^{n}\\delta_i\n\\]\nGiven we know that \\(\\delta_i\\) does not depend on \\(\\theta\\), the last term can be ignored and the likelihood then becomes same as in the discrete case."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#binomial-example",
    "href": "posts/likelihood_primer/index.html#binomial-example",
    "title": "Likelihood",
    "section": "Binomial example",
    "text": "Binomial example\nLet’s consider a binomially distributed random variable \\(Y \\sim B(n,p)\\) (Note: here \\(p\\) is the parameter of interest).  Its likelihood is:\n\\[\nL(p|y) = \\binom{n}{y}p^y(1-p)^{n-y}\n\\]\nThe maximum likelihood estimate (MLE) of \\(p\\) results in the largest probability of observed data. The value at which the maximum occurs is called the maximum likelihood estimate and is denoted as \\(\\hat{p}\\).\nOften, it is simpler to maximise the log of likelihood function \\(l(\\theta|y) = log(L(\\theta|y))\\), considering that log is a monotonic increasing function so maximising either should result in the exact same estimate \\(\\hat{\\theta}\\).\nTo find the solution for \\(\\hat{p}\\) in our binomial case, we can start by defining the log-likelihood:\n\\[\nl(p|y) = log(\\binom{n}{y}) + ylog(p) + (n-y)log(1-p)\n\\]\nA typical calculus approach to maximising this function would be to take the derivative of this log-likelihood w.r.t. parameters of interest and set it to 0.  Worth noting that the derivative is called score function and denoted as \\(u(\\theta)\\) in general. For binomial we have:\n\\[\nu(p) = \\frac{d}{dp}l(p|y) = \\frac{y}{p} - \\frac{n-y}{1-p}\n\\]\nTo get the maximum likelihood estimate \\(\\hat{p}\\) we solve for \\(u(p) = 0\\) and get \\(\\hat{p} = y/n\\)."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#fisher-information-uncertainty",
    "href": "posts/likelihood_primer/index.html#fisher-information-uncertainty",
    "title": "Likelihood",
    "section": "Fisher Information (uncertainty)",
    "text": "Fisher Information (uncertainty)\nSome measure of uncertainty is desirable here to be able to rely on the estimate. This is given by Fisher information:\n\\[\nI(\\theta) = var(u(\\theta)) = -\\mathbb{E}[\\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T}l(\\theta)]\n\\]\nThe double derivative part here implies the rate of change of score itself.  At (or near) the maximum values of log-likelihood, a large value of \\(I(\\theta)\\) effectively indicates a large rate of change of score. Which means that the maximum is clearly defined (curvature of log-likelihood is large).\nUnder regularity conditions, the maximum likelihood estimator (MLE) of the parameter \\(\\theta\\) , denoted as \\(\\hat{\\theta}\\) , is asymptotically unbiased and normally distributed. That is: \\[\n\\hat{\\theta} \\sim \\mathcal{N}\\left(\\theta, \\frac{1}{I(\\theta)}\\right).\n\\] and this means that \\(var(\\hat{\\theta}) = I^{-1}(\\theta)\\). Which gives us the uncertainty on parameter estimate itself.  This makes intuitive sense, when Fisher information is high (clearly defined maximum and high curvature of log-likelihood), then the uncertainty in parameter estimate should be low.\n\nFisher information for Binomial\nNow, in the Fisher information, the expectation is w.r.t. data. Consider the following for binomial distributed data:\n\\[\\begin{align}\n    \\frac{d}{dp}l(p|y) = \\frac{y}{p} - \\frac{n-y}{1-p} \\\\\n    \\frac{d^2}{dp^2}l(p|y)l(p|y) = -\\frac{y}{p^2} - \\frac{n-y}{(1-p)^2} \\\\\n    \\mathbb{E}[\\frac{d^2}{dp^2}l(p|y)l(p|y)] = \\mathbb{E}[-\\frac{y}{p^2} - \\frac{n-y}{(1-p)^2}] \\\\\n    and, \\mathbb{E}[y] = np \\\\\n    \\mathbb{E}[\\frac{d^2}{dp^2}l(p|y)l(p|y)] = -\\frac{n}{p} - \\frac{n}{(1-p)} \\\\\n    I(p) = - \\mathbb{E}[\\frac{d^2}{dp^2}l(p|y)l(p|y)] = \\frac{n}{p} + \\frac{n}{(1-p)} \\\\\n    I(p) = \\frac{n}{p(1-p)}\n\n\\end{align}\\]\nSo, the larger the \\(n\\) (sample size) the greater is Fisher information and lesser uncertainty about the maxima point of score function. Similarly, for same \\(n\\) more extreme values of \\(p\\) will enable estimation of parameter with greater degree of confidence due to higher Fisher Information. Intuitively, this is because extreme values in data will be observed (lots of 1s or lots or 0s).\n\n\n\n\n\n\n\n\n\n\n\nEffect of Fisher information on log-likelihood\nWe can also illustrate the effect on score (log-likelihood) becoming more defined with higher Fisher information (due to higher \\(n\\)) with same choices as above plot.  Essentially we consider 2 binomial datasets one with \\(n=25, y=10\\) and another with \\(n=50, y=20\\) where \\(y\\) is observed counts of successes.\n\nloglik &lt;- function(x, y, n) lchoose(n, y) + y * log(x) + (n - y) * log(1 - x)\n\ncurve(\n  loglik(x, y = 10, n = 25),\n  0,\n  1,\n  lty = 1,\n  xlab = 'p',\n  ylab = 'Log-likelihood',\n  main = 'Log-likelihood of Binomial distribution'\n)\ncurve(loglik(x, y = 20, n = 50), 0, 1, add = TRUE, lty = 2)\ngrid()\nlegend(\"topright\", legend = c(\"n=25\", \"n=50\"), lty = c(1, 2))\n\n\n\n\n\n\n\n\nThe maximum can be noticed at \\(p=0.4\\) (equal to \\(n/y\\) in both cases as derived earlier). For larger sample size \\(n=50\\) the curvature of log-likelihood is greater as there is more information available.  For our binomial example we maximised the log-likelihood function analytically earlier. It was a convenient exercise, however it is not always possible to analytically maximise log-likelihood and estimate parameters."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#numerical-estimation",
    "href": "posts/likelihood_primer/index.html#numerical-estimation",
    "title": "Likelihood",
    "section": "Numerical estimation",
    "text": "Numerical estimation\nTypically numerical optimisation is necessary when estimating parameters using log-likelihood maximisation. The Newton-Raphson method is applied for these purposes. Consider \\(\\theta_0\\) as an initial guess for \\(\\theta\\), then an update is made using: \\[\n\\theta_1 = \\theta_0 - H^{-1}(\\theta_0) J(\\theta_0)\n\\] Where,  \\(H(\\theta) = \\frac{\\partial^2}{\\partial \\theta \\partial \\theta^T}l(\\theta)\\) is the Hessian matrix of second derivatives of log-likelihood w.r.t. \\(\\theta\\) .  \\(J(\\theta) = \\frac{d}{d\\theta}l(\\theta)\\) is the Jacobian matrix of derivatives of log-likelihood w.r.t. \\(\\theta\\) .\nThis method works well, provided the log-likelihood is smooth and convex around the maximum and the initial value is reasonably chosen, but is not a guaranteed solution for all maximisation problems (such as when maximum is on boundary of parameter space or when there are several local maximums). \n\nNormal example\nTo develop intuition on this method consider a Normal distribution: \\[\nf(x \\mid \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)\n\\]\nIts log-likelihood can be written as \\[\nl(\\mu) = -\\frac{n}{2} \\log(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2.\n\\]\nWe calculate the first and second derivatives of log-likelihood\n\\[\\begin{align}\nl'(\\mu) = \\frac{\\partial l(\\mu)}{\\partial \\mu} = \\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu) \\\\\n\nl''(\\mu) = \\frac{\\partial^2 l(\\mu)}{\\partial \\mu^2} = -\\frac{n}{\\sigma^2}\n\n\\end{align}\\]\nThen we perform the Newton Raphson update\n\\[\\begin{align}\n\\mu^{(t+1)} = \\mu^{(t)} - \\frac{l'(\\mu^{(t)})}{l''(\\mu^{(t)})} \\\\\n\\mu^{(t+1)} = \\mu^{(t)} - \\frac{\\frac{1}{\\sigma^2} \\sum_{i=1}^n (x_i - \\mu^{(t)})}{-\\frac{n}{\\sigma^2}} \\\\\n\\mu^{(t+1)} = \\mu^{(t)} + \\frac{\\sum_{i=1}^n (x_i - \\mu^{(t)})}{n} \\\\\n\\mu^{(t+1)} = \\frac{1}{n} \\sum_{i=1}^n x_i\n\\end{align}\\]\nThus, in this case, the Newton-Raphson method directly converges to the sample mean \\(\\bar{x}\\) , which is the maximum likelihood estimate (MLE) for \\(\\mu\\).\n\n\nBinomial solution\nLet’s now revert back to our Binomial distribution setting. We can perform log-likelihhod maximisation with Newton Raphson using R. Note that we need to minimize \\(-l(\\theta)\\) because nlm minimizes and doesn’t maximize.\n\nn &lt;- 25\ny &lt;- 10\nf &lt;- function(x) -loglik(x, y, n)\nmle &lt;- nlm(f, 0.5, hessian = TRUE)\n\nWarning in log(x): NaNs produced\n\n\nWarning in nlm(f, 0.5, hessian = TRUE): NA/Inf replaced by maximum positive\nvalue\n\n\nWarning in log(x): NaNs produced\n\n\nWarning in nlm(f, 0.5, hessian = TRUE): NA/Inf replaced by maximum positive\nvalue\n\n\nWe use an initial value of \\(p=0.5\\) and set hessian to TRUE to be able to calculate standard errors of the estimate using it.\nAt the MLE \\(\\hat{\\theta}\\) , the Hessian \\(H(\\hat{\\theta})\\) is often used as an estimate for the Fisher information. (since both are effectively double derivative of log-likelihood). We have \\[\nI(\\hat{\\theta}) \\approx -H(\\hat{\\theta})\n\\] We have seen earlier that the variance of parameter is the inverse of Fisher information, which gives us \\[\n\\text{Var}(\\hat{\\theta}) = I^{-1}(\\theta) \\approx [-H(\\hat{\\theta})]^{-1}\n\\]\nHence, the inverse of the Hessian at the optimum is equal to the variance of the estimate.  Note: - sign is excluded since we’re working with -log-likelihood in nlm\n\nprint(paste(\"Numerical estimate of p:\", round(mle$estimate, 2)))\n\n[1] \"Numerical estimate of p: 0.4\"\n\nprint(paste(\"Estimated variance of p:\", round(1 / mle$hessian, 4)))\n\n[1] \"Estimated variance of p: 0.0096\"\n\nprint(paste(\"Actual variance variance of p:\", round(0.4 * (1 - 0.4) / n, 4)))\n\n[1] \"Actual variance variance of p: 0.0096\"\n\n\nThis is a useful toolkit to have when working with estimations and maximum likelihood."
  },
  {
    "objectID": "posts/likelihood_primer/index.html#wrap-up",
    "href": "posts/likelihood_primer/index.html#wrap-up",
    "title": "Likelihood",
    "section": "Wrap up",
    "text": "Wrap up\nBy understanding these fundamental concepts, one can gain deeper insights into statistical inference and model fitting, helping to make more informed decisions in real-world applications."
  },
  {
    "objectID": "posts/movies_getting_longer/index.html",
    "href": "posts/movies_getting_longer/index.html",
    "title": "Movie runtime ~ year, analysis",
    "section": "",
    "text": "I recently came across an article posing a question:\n\nAre blockbusters getting (reely) longer?\n\nFueled by the buzz around Christopher Nolan’s Oppenheimer being his longest movie (just over 3 hours), I decided to explore this question using publicly available data.\nAs an exercise in inference this question can be answered with some analysis. I fetched top grossing movies in each year from boxoffice mojo and movie details from imdb and combined them to create the following dataset.\ndata"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#introduction",
    "href": "posts/movies_getting_longer/index.html#introduction",
    "title": "Movie runtime ~ year, analysis",
    "section": "",
    "text": "I recently came across an article posing a question:\n\nAre blockbusters getting (reely) longer?\n\nFueled by the buzz around Christopher Nolan’s Oppenheimer being his longest movie (just over 3 hours), I decided to explore this question using publicly available data.\nAs an exercise in inference this question can be answered with some analysis. I fetched top grossing movies in each year from boxoffice mojo and movie details from imdb and combined them to create the following dataset.\ndata"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#data",
    "href": "posts/movies_getting_longer/index.html#data",
    "title": "Movie runtime ~ year, analysis",
    "section": "Data",
    "text": "Data\n\n\n# A tibble: 6 × 3\n  release_year runtime_mins title                       \n         &lt;dbl&gt;        &lt;dbl&gt; &lt;chr&gt;                       \n1         1990          103 Home Alone                  \n2         1990          127 Ghost                       \n3         1990          181 Dances with Wolves          \n4         1990          119 Pretty Woman                \n5         1990           93 Teenage Mutant Ninja Turtles\n6         1990          135 The Hunt for Red October"
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#analysis",
    "href": "posts/movies_getting_longer/index.html#analysis",
    "title": "Movie runtime ~ year, analysis",
    "section": "Analysis",
    "text": "Analysis\n\nComparing recent vs old movies\nTo simplify the analysis we can consider more recent releases (2013-2023) and compare them to releases from much older timeframe (1990-2000).  The runtime distributions for movies in these 2 categories is different although not significantly (visually).\n\n\n\n\n\n\n\n\n\nNow we can compare runtimes between two periods. It would have been a straightforward exercise in testing the hypothesis of difference of means using a t-test had the distributions of runtimes been more normally distributed. \nAdditionally, the cutoff chosen for old (&lt;= 2000) and recent (&gt;=2013) are arbitrary. We would ideally want a more comprehensive statement about movie runtimes increasing over the years. \nNeverthless, as a preliminary (and simple) analysis we can use bootstrap to compare means of runtimes in the above groups.\n\n\n\nORDINARY NONPARAMETRIC BOOTSTRAP\n\n\nCall:\nboot(data = testset, statistic = diff_mean, R = 1000)\n\n\nBootstrap Statistics :\n    original     bias    std. error\nt1* 9.697248 0.06394496    3.174053\n\n\n\n\n\n\n\n\n\nBOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS\nBased on 1000 bootstrap replicates\n\nCALL : \nboot.ci(boot.out = bootstrap_results, type = \"perc\")\n\nIntervals : \nLevel     Percentile     \n95%   ( 3.502, 15.736 )  \nCalculations and Intervals on Original Scale\n\n\nIf we bootstrap the mean of movie times and take the difference between means of recent release years from old, there is evidence that blockbuster movies are getting longer in recent years, and the difference can be up to 10 min on average and can be expected to be between 4 min and 16 min. \nSo on your next visit to the theatre make sure to get some extra popcorn !!!\n\n\nEstimating effect of time\nIn the want of better precision in identifying the effect of time on movie runtimes, we can formulate a regression problem and estimate the effect of year.\n\n\n\n\n\n\n\n\n\nThe plot above partly confirms our intuition about the increasing trend of runtime.\n\n\n\nCall:\nlm(formula = runtime_mins ~ release_year, data = yearly_top_movies)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-43.369 -19.617   0.488  16.422  77.834 \n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)  -636.1435   271.4520  -2.343  0.01969 * \nrelease_year    0.3791     0.1353   2.802  0.00537 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 24.42 on 335 degrees of freedom\nMultiple R-squared:  0.0229,    Adjusted R-squared:  0.01999 \nF-statistic: 7.852 on 1 and 335 DF,  p-value: 0.005372\n\n\nThe regression above supports the hypothesis that release_year has positively contributed to movie runtimes at approximately 0.38 min/year.  Considering our previous finding on average between 1995 and 2018 the runtime increased by approx. 10 min. From the regression estimate the increase is 0.3791 * (2018-1995) = 8.7 min. \nWe can certainly run some diagnostics to be sure of our model.\n\n\n\n\n\n\n\n\n\nThe residuals appear to have no pattern with fitted values and also have fairly constant variance. There are no significantly high leverage points or outliers.  However, the errors are not quite normally distributed.  This can be ignored by relying on large sample size and the fact that other diagnostics are fine.  But, we shall check if just in case any transformation of runtime_mins may help.\n\n\n\n\n\n\n\n\n\nThe Box-Cox transformation check above hints that there may be some benefit if we used log-transformation (since lambda approximately 0). This thread does not materialise into anything meaningful as upon transformation, the diagnostics do not change materially and the effect size of release_year is practically the same (0.32% per year, 0.38 min/year on base of 119 min). So we will not use any transformation in our model.\nThere may also be some degree of autocorrelation among residuals due to the fact that we’re working with timeseries data. We can test for this autocorrelation and handle it if necessary.\n\n\nAuto-correlation among residuals:  0.01\n\n\n\n    Durbin-Watson test\n\ndata:  runtime_mins ~ release_year\nDW = 1.982, p-value = 0.4126\nalternative hypothesis: true autocorrelation is greater than 0\n\n\nThere doesn’t appear to be significant autocorrelation and other model diagnotics appear to be fine."
  },
  {
    "objectID": "posts/movies_getting_longer/index.html#conclusion",
    "href": "posts/movies_getting_longer/index.html#conclusion",
    "title": "Movie runtime ~ year, analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThere has been a rather steady increase of 0.38 min/year (on average) in movie runtimes as per the dataset we have used. In effect expect to be seated for about 10 more minutes in your favourite blockbuster compared to when you went to movies as a kid."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html",
    "href": "posts/user_behaviour_analysis/index.html",
    "title": "Probabilistic model of User journey",
    "section": "",
    "text": "Duolingo have been making waves with their customer success stories and strong (and growing) user base. They seem to be taking their metrics seriously and have done a great job explaining their approach to understanding user behaviour in this (Gustafson 2024).  In the current blog, I wanted to apply their methodology and use it for predicting future customer behaviours. \nThe problem statement driving this analysis is simple. Duolingo want to predict their future daily active users (DAU) with some control handles that they can manipulate to improve those numbers e.g. send reminders to at risk weekly active users or reward campaigns to at risk monthly active users.  They may be able to run (or have already run) experiments identifying the effects of these campaigns and reminder in terms of % of people that respond to the treatment and revert back to being daily active.  This allows them to efficiently allocate resources and maintain a healthy user base that eventually materialises into paying customers."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#introduction",
    "href": "posts/user_behaviour_analysis/index.html#introduction",
    "title": "Probabilistic model of User journey",
    "section": "",
    "text": "Duolingo have been making waves with their customer success stories and strong (and growing) user base. They seem to be taking their metrics seriously and have done a great job explaining their approach to understanding user behaviour in this (Gustafson 2024).  In the current blog, I wanted to apply their methodology and use it for predicting future customer behaviours. \nThe problem statement driving this analysis is simple. Duolingo want to predict their future daily active users (DAU) with some control handles that they can manipulate to improve those numbers e.g. send reminders to at risk weekly active users or reward campaigns to at risk monthly active users.  They may be able to run (or have already run) experiments identifying the effects of these campaigns and reminder in terms of % of people that respond to the treatment and revert back to being daily active.  This allows them to efficiently allocate resources and maintain a healthy user base that eventually materialises into paying customers."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#methodology",
    "href": "posts/user_behaviour_analysis/index.html#methodology",
    "title": "Probabilistic model of User journey",
    "section": "Methodology",
    "text": "Methodology\nFirst we shall model the process as a Markov chain to see how well the approach predicts future user counts.  Subsequently, to validate MC approach and rethink the problem setup in want of simplification, we shall look to model the time-series as a regression problem. We shall do this respecting the mechanics of transition and see how far this can get us in terms of analysing and controlling the process.\nAt day d (d = 1, 2, … ) of a user’s lifetime, the user can be in one of the following 7 (mutually-exclusive) states: \n\nnew : learners who are experiencing Duolingo for the first time ever\ncurrent : learners active today, who were also active in the past week\nreactivated : learners active today, who were also active in the past month (but not the past week)\nresurrected : learners active today, who were last active &gt;30 days ago\nat_risk_wau (at risk weekly active users) : learners who have been active within the past week, but not today\nat_risk_mau (at risk monthly active users) : learners who were active within the past month, but not the past week\ndormant : learners who have been inactive for at least 30 days\n\nA brief overview of how these states are related is below:\n\n\n\nUser states and transitions\n\n\nThe transition acronyms are defined below:\n\nNURR : New User Retention Rate (The proportion of day 1 learners who return on day 2)\nCURR : Current User Retention Rate (This is not a 100%. Not all of current users stay active every day since people forget to complete a lesson, or have technical difficulties, or just want to take a break)\nRURR : Reactivated User Retention rate (The proportion of at_risk_mau who became active today)\nSURR : Resurrected User Retention rate (The proportion of dormant users who became active today)\niWAURR : The proportion of at_risk_wau users who became active today\n\nAs is evident, these quantities (along with others) are what will become the elements of Transition matrix (M), that we discuss next.\nIn keeping with the definition chart above, the next step is to consider user behaviour as a Markov chain.  Let M be a transition matrix associated with this Markov process: m(i, j) = P(s_j | s_i) are the probabilities that a user moves to state s_j right after being at state s_i. The matrix M is learned from the historical data.\nAssuming that user behavior is stationary (independent of time), the matrix M fully describes the states of all users in the future.\nSuppose that the vector u_0 of length 7 contains the counts of users in certain states on a given day (say day 0). According to the Markov model, on the next day (day 1), we expect to have the following number of users (u_1) in respective states:\n\n\n\nState transition counts estimation\n\n\nApplying this multiplication recursively, we can derive the number of users in any states on any arbitrary day t &gt; 0 in the future (call this vector u_t).\nNow, having u_t calculated, we can determine DAU, WAU and MAU values on day t:\n\nDAU_t = #New_t + #Current_t + #Reactivated_t + #Resurrected_t\nWAU_t = DAU_t + #AtRiskWau_t\nMAU_t = DAU_t + #AtRiskWau_t + #AtRiskMau_t\n\nFinally, here’s the algorithm outline:\n\nFor each prediction day t = 1, …, T, calculate the expected number of new users #New_1, …, #New_T.\nFor each lifetime day of each user, assign one of the 7 states.\nCalculate the transition matrix M from the historical data.\nCalculate initial state counts u_0 corresponding to day t=0.\nRecursively calculate u_{t+1} = M^t * u_0.\nCalculate DAU, WAU, and MAU for each prediction day t = 1, …, T."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#implementation",
    "href": "posts/user_behaviour_analysis/index.html#implementation",
    "title": "Probabilistic model of User journey",
    "section": "Implementation",
    "text": "Implementation\n\nData\nWe use a simulated dataset based on historical data of a SaaS app.  The data is available here and contains three columns: user_id, date, and registration_date.\nEach record indicates a day when a user was active. \n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\nd8c465ab-e9fd-5edd-9e4e-c77094700cb5\n2020-10-01\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-01\n2020-08-25\n\n\nbfeac474-5b66-566f-8654-262bb79c873e\n2020-10-01\n2020-05-31\n\n\nd32fcac5-122c-5463-8aea-01b39b9ad0bb\n2020-10-01\n2020-09-30\n\n\nc1ece677-e643-5bb3-8701-f1c59a0bf4cd\n2020-10-01\n2020-09-05\n\n\nba6750c9-9b60-58dc-8d40-504e7375249e\n2020-10-01\n2020-09-29\n\n\n\n\n\n\n\nTotal users: 51480\n\n\nDate range: 2020-10-01 to 2023-10-31\n\n\nThis is how DAU timeseries looks\n\n\n\n\n\n\n\n\n\n\n\nFuture new user count model\nTo even begin thinking about transitions on a future date, we first need to acknowledge the fact that new user counts will be a significant missing piece of information in the analysis.  To get around this limitation, we shall first build a new user count prediction model that need not be very accurate (since in a realistic setting new user counts can also be a parameter that can be changed).\nThe information on new users on any given date is inherent in registration_date in the data.\nAfter some experimentation, the model setup to predict future counts of new users is as above.  The counts are assumed Poisson but with a higher variance than normal due to spikes in the data and hence a negative binomial model has been used.  It was imperative that we used a robust version of GLM to be able to get relatively constant variance of errors, that were being caused due to outliers in user count towards beginning of every year.  The model appears to be doing a satisfactory job of predicting counts of new users and we consider this acceptable for further analysis.\n\n\n\n\n\n\n\n\n\n\n\nAssigning states\nThere needs to be an assignment of one of the 7 states (mentioned earlier) against each date for a user. This will help identify transitions made between states and populate the Transition matrix. However, the data only consists of records of active days, we need to explicitly extend them and include the days when a user was not active. In other words, instead of this table of records:\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n1234567\n2023-01-01\n2023-01-01\n\n\n1234567\n2023-01-03\n2023-01-01\n\n\n\nwe’d like to get a table like this:\n\n\n\nuser_id\ndate\nis_active\nregistration_date\n\n\n\n\n1234567\n2023-01-01\nTRUE\n2023-01-01\n\n\n1234567\n2023-01-02\nFALSE\n2023-01-01\n\n\n1234567\n2023-01-03\nTRUE\n2023-01-01\n\n\n1234567\n2023-01-04\nFALSE\n2023-01-01\n\n\n…\n…\n…\n…\n\n\n1234567\n2023-10-31\nFALSE\n2023-01-01\n\n\n\nAnd using this table, subsequently estimate states using simple rules (mentioned in definitions above) for each user_id, corresponding to each day in the data.\nThe volume of data at our disposal makes this a challenging task to run conveniently on a small machine, so the state determination is performed offline and the code to do it can be found here\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nis_active\nstate\nregistration_date\n\n\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-18\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-19\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-20\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-21\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-22\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-23\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-24\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-25\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-26\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-27\nFALSE\nat_risk_mau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-28\nTRUE\nreactivated\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-29\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-30\nTRUE\ncurrent\n2020-08-25\n\n\n\n\n\n\n\nCalculating transition matrix\nWe need a flexible approach to calculate transition matrix over any arbitrary period of time. This is because it is likely that transition matrix changes over time as user preferences and product itself evolves. \n\nget_transition_matrix &lt;- function(states_slice) {\n        M &lt;- states_slice %&gt;%\n            group_by(user_id) %&gt;%\n            arrange(date) %&gt;%\n            mutate(\n                state_to = lead(state),\n                state_from = state\n            ) %&gt;%\n            ungroup() %&gt;%\n            # Remove the last row since user doesn't transition to anywhere from that\n            filter(!is.na(state_to)) %&gt;%\n            # Count transitions per user to avoid counting transitions across users\n            group_by(user_id, state_from, state_to) %&gt;%\n            summarise(transition_count = n()) %&gt;%\n            ungroup() %&gt;%\n            # Sum all the transitions over all users\n            group_by(state_from, state_to) %&gt;%\n            summarise(transition_count = sum(transition_count)) %&gt;%\n            ungroup() %&gt;%\n            # Generates a wide form matrix from long form\n            pivot_wider(names_from = state_to, values_from = transition_count, values_fill = list(transition_count = 0)) %&gt;%\n            # Normalizes matrix of counts so that rows sum to 1\n            mutate(row_sum = rowSums(dplyr::select(., -state_from))) %&gt;%\n            mutate(across(-state_from, ~ . / row_sum)) %&gt;%\n            dplyr::select(-row_sum) %&gt;%\n            # Generates ordered matrix where row and column name orders match\n            mutate(new = 0) %&gt;% # To get consistent matrix since 'new' is missing from state_to\n            column_to_rownames(\"state_from\") %&gt;%\n            .[order(rownames(.)), order(colnames(.))]\n    return(M)\n}\nEssentially we seek to calculate a matrix which has state_from in its rows state_to as its column and fraction of transitions as values. All the values in a row must sum to 1 obeying the fact that from a given state a user will end up in one of the 7 states.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat_risk_mau\nat_risk_wau\ncurrent\ndormant\nnew\nreactivated\nresurrected\n\n\n\n\nat_risk_mau\n0.9520158\n0.0000000\n0.0000000\n0.0384384\n0\n0.0093866\n0.0001591\n\n\nat_risk_wau\n0.1308719\n0.7663644\n0.0982922\n0.0000000\n0\n0.0044715\n0.0000000\n\n\ncurrent\n0.0000000\n0.1487694\n0.8512306\n0.0000000\n0\n0.0000000\n0.0000000\n\n\ndormant\n0.0000000\n0.0000000\n0.0000000\n0.9996200\n0\n0.0000000\n0.0003800\n\n\nnew\n0.0000000\n0.4836773\n0.5163227\n0.0000000\n0\n0.0000000\n0.0000000\n\n\nreactivated\n0.0000000\n0.6353907\n0.3646093\n0.0000000\n0\n0.0000000\n0.0000000\n\n\nresurrected\n0.0000000\n0.6848495\n0.3151505\n0.0000000\n0\n0.0000000\n0.0000000\n\n\n\n\n\nAs a sanity check, we can analyse the matrix itself.  It makes sense that current -&gt; current transition value is high. This means that active users tend to continue being active with high probability or otherwise become at_risk_wau.  It also seems sensible that there is no state from which one could transition to new. Apparantly quite a high proportion of dormant users stay dormant which could be one of the focuses of resurrection campaigns in such companies.\n\n\nPredicting DAU\nWe start with getting initial state counts for the date on which the dataset ends. This combined with M transition matrix should give us new state counts. We will be mindful to update new_user_count for all the days we would like to predict on. This is done using the prediction model developed earlier.\nWe shall define a flexible mechanism to predict DAU, to enable changing prediction horizon. We will also supply a dataframe of new_users to this mechanism so that it may use predicted values for appropriate prediction dates.\n\npredict_dau &lt;- function(M, state0, start_date, end_date, new_users_predicted) {\n    dates &lt;- seq(as.Date(start_date), as.Date(end_date), by = \"day\")\n    # Align state0 name order with transition Matrix name order\n    new_dau &lt;- state0[match(state0$state, rownames(M)), ]$cnt\n    dau_pred &lt;- list()\n\n    for (dt in dates) {\n        new_dau &lt;- as.integer(t(M) %*% new_dau) # Transitions\n        new_users_today &lt;- new_users_predicted %&gt;%\n            filter(date == dt) %&gt;%\n            pull(predicted) %&gt;%\n            as.integer()\n        new_dau[5] &lt;- new_users_today # Today's new users get transitioned next day\n        dau_pred &lt;- append(dau_pred, list(new_dau))\n    }\n\n    df &lt;- data.frame(do.call(rbind, dau_pred))\n    colnames(df) &lt;- rownames(M)\n    df$date &lt;- dates\n    df$dau &lt;- df$new + df$current + df$reactivated + df$resurrected\n    df$wau &lt;- df$dau + df$at_risk_wau\n    df$mau &lt;- df$dau + df$at_risk_wau + df$at_risk_mau\n\n    return(tibble(df))\n}\n\n\n\n\n\n\n\n\n\nThe prominent contributer to DAU counts are current users, who are aided by addition of new users everyday and the fact that current users stay active with a probability of 85% (from M matrix).  The prediction makes intuitive sense however the magnitude of spike certainly seems quite large.  Observing the shape of predicted DAU, it seems quite strongly influenced by predicted new user count from the model earlier. Additionally, some degree of experimentation with the predictions suggest that about 20% decrease in new user counts bring predicted DAU to comparable levels of previous years (dotted line in the plot)."
  },
  {
    "objectID": "posts/teen_gamble/index.html",
    "href": "posts/teen_gamble/index.html",
    "title": "gambling spend ~ gender, analysis",
    "section": "",
    "text": "Teenage gambling can take many forms—scratch cards, sports bets, online games—and even small amounts can add up over a year. Understanding who spends and why can help families, schools, and policymakers support young people.\nThis post analyses gambling spending in a British survey of adolescents. We focus on two simple questions: \n\nDo boys and girls spend different amounts on gambling?\nIs having more weekly spending money linked to higher annual gambling?\n\nWe keep things straightforward: we describe the data, compare boys and girls, and account for weekly income so the comparison is fair. Because many teenagers report little or no gambling while a few report much higher amounts, we analyze a gently transformed version of annual spending that handles zeros (no spend) and very large spend values sensibly. We also set aside one unusual observation so it does not distort the findings, and we run basic checks to make sure the results are not driven by modeling quirks. The analysis is descriptive and it looks for patterns, not proof of cause and effect."
  },
  {
    "objectID": "posts/teen_gamble/index.html#introduction",
    "href": "posts/teen_gamble/index.html#introduction",
    "title": "gambling spend ~ gender, analysis",
    "section": "",
    "text": "Teenage gambling can take many forms—scratch cards, sports bets, online games—and even small amounts can add up over a year. Understanding who spends and why can help families, schools, and policymakers support young people.\nThis post analyses gambling spending in a British survey of adolescents. We focus on two simple questions: \n\nDo boys and girls spend different amounts on gambling?\nIs having more weekly spending money linked to higher annual gambling?\n\nWe keep things straightforward: we describe the data, compare boys and girls, and account for weekly income so the comparison is fair. Because many teenagers report little or no gambling while a few report much higher amounts, we analyze a gently transformed version of annual spending that handles zeros (no spend) and very large spend values sensibly. We also set aside one unusual observation so it does not distort the findings, and we run basic checks to make sure the results are not driven by modeling quirks. The analysis is descriptive and it looks for patterns, not proof of cause and effect."
  },
  {
    "objectID": "posts/teen_gamble/index.html#data",
    "href": "posts/teen_gamble/index.html#data",
    "title": "gambling spend ~ gender, analysis",
    "section": "Data",
    "text": "Data\nThe analysis uses data from (Ide-Smith and Lea 1988), a survey of teenage gambling behaviour in Britain. \nThe key variables are:\n\nsex\nstatus : Socioeconomic status score based on parents’ occupation\nincome : Weekly income in pounds\nverbal : Verbal score (number of words defined correctly, out of 12)\ngamble : Annual expenditure on gambling in pounds\n\n\n\n\n\n\nsex\nstatus\nincome\nverbal\ngamble\n\n\n\n\nfemale\n51\n2.00\n8\n0.0\n\n\nfemale\n28\n2.50\n8\n0.0\n\n\nfemale\n37\n2.00\n6\n0.0\n\n\nfemale\n28\n7.00\n4\n7.3\n\n\nfemale\n65\n2.00\n8\n19.6\n\n\nfemale\n61\n3.47\n6\n0.1\n\n\n\n\n\nA cursory look at the data suggests that females likely spend less on gambling compared to males. Additionally, most females do not spend more than approximately £20 per year on gambling.\n\n\n\n\n\n\n\n\n\nThere also seems positive association between income and amount spent on gambling (at least for males). This is not very surprising as one would expect that individuals with higher income would be able to spend more on gambling.  From what this plot indicates, it may be worth evaluating if there is a differential effect of income based on gender."
  },
  {
    "objectID": "posts/teen_gamble/index.html#analysis",
    "href": "posts/teen_gamble/index.html#analysis",
    "title": "gambling spend ~ gender, analysis",
    "section": "Analysis",
    "text": "Analysis\nThe response variable gamble is non‑negative and strongly right‑skewed. We therefore model log(gamble + 1). Adding £1 ensures the log is defined for zero spenders, keeps small expenditures near zero on the transformed scale, and avoids introducing artificial extreme negative values. Note this is different from a GLM with a log link: here we model the (logged) response directly, which changes how effects are interpreted on the original scale. \nObservation 24 (gamble = £156) appears to be an outlier and is removed from the analysis.\n\nEffect of Gender\nIt seems plausible that males are spending more on gambling compared to females. This is also indicated by the boxplot below. We will evaluate this hypothesis using a simple linear model with log(gamble + 1) as response and sex as explanatory variable.\n\n\n\n\n\n\n\n\n\nAnalysis of Variance Table\n\nResponse: log_gamble\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nsex        1 20.365  20.365  12.261 0.001074 **\nResiduals 44 73.085   1.661                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nGender seems to have statistically significant association with gambling expenditure.  Anova procedure is equivalent to a t-test with constant variance assumption (Faraway 2025), which seems fair from the Boxplot above. They test for the same effect and result in identical p-values. \nBefore moving to analysing joint effect of Income and Gender, it is imperative to check if the effect of gender is not due to confounding by income. It could well be the case that individuals with higher income spend more on gambling. But then if high income itself is associated with gender, then the above effect of gender may be spurious. We will explore this next.\n\n\n\n\n\n\n\n\n\nAnalysis of Variance Table\n\nResponse: income\n          Df Sum Sq Mean Sq F value Pr(&gt;F)\nsex        1   4.58  4.5754  0.3685 0.5469\nResiduals 44 546.25 12.4149               \n\n\nThe above plot and the test indicates that income is not significantly associated with gender and hence does not mask its effect on gambling expenditure. \nFor the analysis that follows, we shall print the summary of gender only model to show the Estimate value and Std. Error of gender.\n\n\n\nCall:\nlm(formula = log_gamble ~ sex, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.4630 -1.0616  0.1547  1.0979  2.0478 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.1117     0.2957   3.760 0.000498 ***\nsexmale       1.3513     0.3859   3.502 0.001074 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.289 on 44 degrees of freedom\nMultiple R-squared:  0.2179,    Adjusted R-squared:  0.2002 \nF-statistic: 12.26 on 1 and 44 DF,  p-value: 0.001074\n\n\n\n\nEffect of Gender and Income\nWith income variable included alongside sex, the effect of sex remains significant (although is reduced from 1.35 to 1.24 with lower Std. Error).  Income itself is significantly associated with gambling expenditure.  The intercept is non-significant indicating that for reference class (female), gambling expenditure is likely 0£ if they have close to 0 income and it makes intuitive sense (although for adults and addicts this may not necessarily be true).  This model does not include an interaction between sex and income for simplicity, yet it fits the data well. Model diagnostics appear acceptable. \n\n\n\nCall:\nlm(formula = log_gamble ~ sex + income, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.2327 -0.7944  0.1318  0.8419  2.2973 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  0.37092    0.33009   1.124 0.267373    \nsexmale      1.23699    0.34216   3.615 0.000782 ***\nincome       0.17852    0.04869   3.667 0.000671 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.138 on 43 degrees of freedom\nMultiple R-squared:  0.4042,    Adjusted R-squared:  0.3765 \nF-statistic: 14.59 on 2 and 43 DF,  p-value: 1.46e-05\n\n\n\n\n\n\n\n\n\nAdditionally, the residuals do not indicate any obvious pattern with respect to gender.\n\n\n\n\n\n\n\n\n\nFrom earlier plots we suspected a possible differential effect of income by gender. Fitting an interaction did not improve fit and the interaction term is not significant, so the simpler additive model is preferred.\n\n\nAnalysis of Variance Table\n\nModel 1: log_gamble ~ sex * income\nModel 2: log_gamble ~ sex + income\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1     42 53.517                           \n2     43 55.676 -1   -2.1582 1.6938 0.2002\n\n\nAdditionally, interaction term is not significant and so it is best to remove it and keep the model simple. \n\n\n\nCall:\nlm(formula = log_gamble ~ sex * income, data = teengamb)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.1894 -0.9791  0.0763  0.8456  2.0447 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)     0.85855    0.49761   1.725   0.0918 .\nsexmale         0.58832    0.60303   0.976   0.3348  \nincome          0.06101    0.10240   0.596   0.5545  \nsexmale:income  0.15114    0.11613   1.301   0.2002  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.129 on 42 degrees of freedom\nMultiple R-squared:  0.4273,    Adjusted R-squared:  0.3864 \nF-statistic: 10.45 on 3 and 42 DF,  p-value: 2.921e-05"
  },
  {
    "objectID": "posts/teen_gamble/index.html#conclusion",
    "href": "posts/teen_gamble/index.html#conclusion",
    "title": "gambling spend ~ gender, analysis",
    "section": "Conclusion",
    "text": "Conclusion\nBoth gender and income are significantly associated with teenage gambling expenditure. Males spend about three times more than females on average (note: reporting biases could inflate this effect; boys may over-report and girls may under-report (Ide-Smith and Lea 1988)). A £1 increase in weekly income is associated with an approximate 19% increase in annual gambling expenditure on the log scale interpretation. The interaction between gender and income was not significant, suggesting the effect of income is similar across genders."
  },
  {
    "objectID": "posts/miles_per_gallon/index.html",
    "href": "posts/miles_per_gallon/index.html",
    "title": "MPG ~ manufacturer, analysis",
    "section": "",
    "text": "Fuel economy matters for consumers and environmental policy. We ask a focused question: after accounting for observable vehicle attributes, do manufacturers differ in typical city fuel economy?\nConceptually, we aim to separate manufacturer level differences from variation attributable to specific models. To that end, we compare manufacturers while adjusting for key covariates and explicitly distinguish manufacturers from model level effects so that inferences about brands are not confounded by the mix of models.\nThe sections that follow describe the data and filtering choices, explore salient patterns, and then fit benchmark and hierarchical models with accompanying diagnostics, before concluding with implications and limitations."
  },
  {
    "objectID": "posts/miles_per_gallon/index.html#introduction",
    "href": "posts/miles_per_gallon/index.html#introduction",
    "title": "MPG ~ manufacturer, analysis",
    "section": "",
    "text": "Fuel economy matters for consumers and environmental policy. We ask a focused question: after accounting for observable vehicle attributes, do manufacturers differ in typical city fuel economy?\nConceptually, we aim to separate manufacturer level differences from variation attributable to specific models. To that end, we compare manufacturers while adjusting for key covariates and explicitly distinguish manufacturers from model level effects so that inferences about brands are not confounded by the mix of models.\nThe sections that follow describe the data and filtering choices, explore salient patterns, and then fit benchmark and hierarchical models with accompanying diagnostics, before concluding with implications and limitations."
  },
  {
    "objectID": "posts/miles_per_gallon/index.html#data",
    "href": "posts/miles_per_gallon/index.html#data",
    "title": "MPG ~ manufacturer, analysis",
    "section": "Data",
    "text": "Data\nThe data come from the mpg dataset (GGplot2 Development Team, n.d.). We focus on front‑wheel‑drive petrol cars in the compact, midsize, and subcompact classes to keep the comparison balanced. Because models were selected based on having new releases across 1999–2008 (used as a proxy for popularity), manufacturers appear in the sample according to availability rather than a strict experimental design. This affects whether manufacturers are best treated as random or fixed effects.\nOn one hand, manufacturers could be treated as random effects because they were not experimentally selected; on the other hand, we are interested in manufacturer‑specific differences, so treating manufacturers as fixed effects would be useful if each manufacturer is well represented. After filtering the data some manufacturers are missing or sparsely represented, so the scope of inference is limited — we can test whether manufacturers differ, but precise fixed‑effect estimates for every manufacturer would be unreliable.\nModels are treated as random and are nested inside manufacturers because the sample contains multiple models per manufacturer and the particular models present are effectively a random sample from all possible models.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\nmpg\nfl\nclass\ntrans\n\n\n\n\ntoyota\ncorolla\n1.8\n2008\n4\n26\nr\ncompact\nauto\n\n\nhonda\ncivic\n1.6\n1999\n4\n24\nr\nsubcompact\nauto\n\n\ntoyota\ncamry solara\n3.3\n2008\n6\n18\nr\ncompact\nauto\n\n\nvolkswagen\njetta\n2.0\n2008\n4\n22\np\ncompact\nauto\n\n\nvolkswagen\npassat\n3.6\n2008\n6\n17\np\nmidsize\nauto\n\n\nhyundai\ntiburon\n2.7\n2008\n6\n17\nr\nsubcompact\nauto\n\n\naudi\na4\n1.8\n1999\n4\n18\np\ncompact\nauto\n\n\naudi\na4\n2.8\n1999\n6\n16\np\ncompact\nauto\n\n\ntoyota\ncamry\n2.4\n2008\n4\n21\nr\nmidsize\nmanual\n\n\nvolkswagen\nnew beetle\n2.0\n1999\n4\n19\nr\nsubcompact\nauto\n\n\n\n\n\nBecause of the way the data is collected, (selection of models which had a new release every year between 1999 and 2008 - this was used as a proxy for the popularity of the car), we need to be careful about how manufacturers are treated in the analysis. On one hand it seems only fair to treat manufacturers as also being random, since these were not specifically selected in the design of experiment (like car model these appear randomly in the data based on popularity criteria) (Faraway 2016). On the other hand, we are interested in the actual differences between manufacturers, so treating them as fixed effects is desirable.  However, to warrant the use of fixed effects for manufacturers, we need to have sufficient data for each manufacturer and post filtering, the data should have all the manufacturers of interest. As can be seen this is not the case. \n\n\n\n\n\n\n\n\n\nIt can be seen that several key manufacturers are missing from the final dataset, consequently we may only be able to answer question like Is the mpg among manufacturers significantly different ? as opposed to What is the average mpg of manufactuers to be able to compare them more directly  Of course, the car models are random in the study design, so we will treat them as such. Although these are nested inside manufacturers. \n\nEDA\nData summary is as follows -\n\n\n\n\n\n\n\n\n\n\n\n\ntotal_rows\ntotal_manufacturers\ntotal_models\nmin_mpg\nmax_mpg\nmean_mpg\ntotal_years\n\n\n\n\n87\n8\n15\n16\n28\n20\n2\n\n\n\n\n\n\nmanufacturer\nmodel\ncounts\nmean_mpg\n\n\n\n\naudi\na4\n7\n18.9\n\n\nchevrolet\nmalibu\n5\n18.8\n\n\nhonda\ncivic\n8\n24.5\n\n\nhyundai\nsonata\n7\n19.0\n\n\nhyundai\ntiburon\n7\n18.3\n\n\nnissan\naltima\n6\n20.7\n\n\nnissan\nmaxima\n3\n18.7\n\n\npontiac\ngrand prix\n5\n17.0\n\n\ntoyota\ncamry\n7\n19.9\n\n\ntoyota\ncamry solara\n7\n19.9\n\n\ntoyota\ncorolla\n5\n25.6\n\n\nvolkswagen\ngti\n5\n20.0\n\n\nvolkswagen\njetta\n6\n19.3\n\n\nvolkswagen\nnew beetle\n2\n20.0\n\n\nvolkswagen\npassat\n7\n18.6\n\n\n\n\nThere are (exactly) two years of data covering 15 models. We treat year as a categorical variable rather than continuous. A short sample for one model is shown below.\n\n\n\n\n\nmanufacturer\nmodel\ndispl\nyear\ncyl\nmpg\nfl\nclass\ntrans\n\n\n\n\naudi\na4\n1.8\n1999\n4\n18\np\ncompact\nauto\n\n\naudi\na4\n1.8\n1999\n4\n21\np\ncompact\nmanual\n\n\naudi\na4\n2.8\n1999\n6\n16\np\ncompact\nauto\n\n\naudi\na4\n2.8\n1999\n6\n18\np\ncompact\nmanual\n\n\naudi\na4\n2.0\n2008\n4\n20\np\ncompact\nmanual\n\n\naudi\na4\n2.0\n2008\n4\n21\np\ncompact\nauto\n\n\naudi\na4\n3.1\n2008\n6\n18\np\ncompact\nauto\n\n\n\n\n\nWe observe multiple rows per model/year because of different displacements and transmission types. The data only contain three distinct cylinder sizes (4, 6, 8), which is captured by displacement, so we remove cyl to simplify the analysis.\n\n\n\n\n\n\n\n\n\nFuel type (regular vs premium) is not central to this study and shows little correlation with city mpg, so we drop the fuel type column as well.\n\n\n\n\n\n\n\n\n\nWe visualize how mpg varies across manufacturers and by other attributes to get an initial sense of the patterns.\n\n\n\n\n\n\n\n\n\nThere does seem to be quite a lot variability in the city mpg among manufacturers.  Although, it is definitely possible that this is driven by different models and their attributes more than manufacturers themselves. \nThe plot below suggests (to intuition) that with larger displacement the mpg reduces. Although there is a hint of nonlinearity in the trend, we shall ignore this initially.  Midsize cars are slightly less efficient (lower mpg) than compact and subcompact cars. This is expected since these are heavier with higher displacement engines.  It is hard to get an accurate read on the effect of transmission and so we shall defer it to the model to figure out.\n\n\n\n\n\n\n\n\n\nThe mpg distribution has a right skew, so we take the logarithm of mpg to stabilize variance and to make regression coefficients interpretable as approximate percentage changes."
  },
  {
    "objectID": "posts/miles_per_gallon/index.html#analysis",
    "href": "posts/miles_per_gallon/index.html#analysis",
    "title": "MPG ~ manufacturer, analysis",
    "section": "Analysis",
    "text": "Analysis\n\nBaseline model\nWe start with a simple linear model that includes manufacturer as a fixed effect along with displacement and year. This fixed‑effects model provides a baseline and helps identify which predictors explain the most variation in log(mpg). Transmission and class become less important after accounting for displacement and manufacturer, so we simplify the model accordingly.\n\n\nAnalysis of Variance Table\n\nResponse: log(mpg)\n             Df  Sum Sq Mean Sq  F value    Pr(&gt;F)    \nmanufacturer  7 0.66676 0.09525  25.9752 &lt; 2.2e-16 ***\ndispl         1 0.43577 0.43577 118.8345 &lt; 2.2e-16 ***\nclass         2 0.01534 0.00767   2.0922    0.1307    \ntrans         1 0.00590 0.00590   1.6102    0.2084    \nyear          1 0.13722 0.13722  37.4189 4.133e-08 ***\nResiduals    74 0.27136 0.00367                       \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIt can be seen that manufacturer and displ are highly significant predictors of log(mpg). As seen in the plot earlier, 1L increase in displacement can reduce MPG by ~15% ()  Transmission is not significant in the presence of other predictors as was the observation from plot earlier.  Class is not significant either, although this may be due to confounding with displacement as suspected before. A formal test of displacement being positively related with class for any given model also validates this finding. \n\n\n\n\n\n\n\n\n\nAnalysis of Variance Table\n\nResponse: displ\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nclass      2 11.759  5.8796 27.0251 1.876e-09 ***\nmodel     13 12.390  0.9531  4.3807 2.222e-05 ***\nResiduals 71 15.447  0.2176                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nConsequently we can simplify the model by removing trans and class, which results in a decent fit to the data and diagnostics are satisfactory.\n\n\n\n\n\n\n\n\n\n\n\nMixed effects model\nBecause models are nested within manufacturers and the specific models in the data are essentially a random sample, we fit mixed‑effects models that include random intercepts for manufacturer and for model nested within manufacturer. This accounts for the correlation between observations from the same model and between models from the same manufacturer.\n\n\nFixed Effects:\n            coef.est coef.se\n(Intercept)  3.33     0.04  \ndispl       -0.15     0.01  \nyear2008     0.08     0.01  \n\nRandom Effects:\n Groups             Name        Std.Dev.\n manufacturer:model (Intercept) 0.05    \n manufacturer       (Intercept) 0.06    \n Residual                       0.05    \n---\nnumber of obs: 87, groups: manufacturer:model, 15; manufacturer, 8\nAIC = -199.6, DIC = -250.4\ndeviance = -231.0 \n\n\nIt can be seen that variance due to manufacturer and models nested inside manufacturer are comparable in magnitudes. The variation in mpg is due to both which is sesnible outcome. \nIt can be tested if including model effects is meaningful using a likelihood ratio test. (Scheipl, Greven, and Kuechenhoff 2008)\n\n\n\n    simulated finite sample distribution of RLRT.\n    \n    (p-value based on 10000 simulated values)\n\ndata:  \nRLRT = 13.167, p-value = 4e-04\n\n\nSo we reject the hypothesis that manufacturer:model nesting effect is not significant. \nAdditionally we can check confidence intervals for variance components. Manufacturer variance intervals do not contain 0.\n\n\nComputing bootstrap confidence intervals ...\n\n\n\n110 message(s): boundary (singular) fit: see help('isSingular')\n\n\n\n\n\n\n2.5 %\n97.5 %\n\n\n\n\n.sig01\n0.0061511\n0.0763115\n\n\n.sig02\n0.0000000\n0.1049620\n\n\n.sigma\n0.0444100\n0.0631230\n\n\n(Intercept)\n3.2513088\n3.4227193\n\n\ndispl\n-0.1756225\n-0.1254008\n\n\nyear2008\n0.0574356\n0.1054150\n\n\n\n\n\n\nDiagnostics\nResidual diagnostics suggest approximate normality and roughly constant variance. Random‑effect Q–Q plots indicate the manufacturer:model random effects are reasonably close to normal; manufacturer effects are also approximately normal.\n\n\n\n\n\n\n\n\n\nWe can also check assumption of normality of random effects. Which holds up reasonably well (particularly for manufacturer:model)."
  },
  {
    "objectID": "posts/miles_per_gallon/index.html#conclusion",
    "href": "posts/miles_per_gallon/index.html#conclusion",
    "title": "MPG ~ manufacturer, analysis",
    "section": "Conclusion",
    "text": "Conclusion\nWe set out to test whether manufacturers differ in city mpg after controlling for other attributes. Accounting for model‑level clustering, year and displacement, we find remaining manufacturer‑level variation in city mpg. In other words, even after controlling for model and engine size, manufacturers differ in typical city mpg. \nThe variability among manufacturers is of the order of +/- 6%, which translates to +/-1.2 mpg on average \nLimitations: this analysis is limited to petrol, front‑wheel‑drive cars in the compact/midsize/subcompact classes. Results should not be generalized beyond these groups without further study. Results are also sensitive to the treatment of manufacturers as random effects due to the sampling design."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Regression Room",
    "section": "",
    "text": "rentership ~ occupation, analysis\n\n\n\nrandom effects\n\nhierarchical models\n\nglm\n\nbinomial\n\n\n\n\n\n\nNov 30, 2025\n\n\nPrateek\n\n23 min\n\n\n\n\n\n\n\n\n\n\n\nMPG ~ manufacturer, analysis\n\n\n\nrandom effects\n\nhierarchical models\n\n\n\n\n\n\nNov 23, 2025\n\n\nPrateek\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\ngambling spend ~ gender, analysis\n\n\n\nregression\n\nANOVA\n\n\n\n\n\n\nOct 16, 2025\n\n\nPrateek\n\n6 min\n\n\n\n\n\n\n\n\n\n\n\nLikelihood\n\n\n\nlikelihood\n\nfisher information\n\nnewton raphson\n\n\n\n\n\n\nDec 28, 2024\n\n\nPrateek\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nProbabilistic model of User journey\n\n\n\nmarkov chain\n\ntimeseries\n\n\n\n\n\n\nDec 18, 2024\n\n\nPrateek\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\nMovie runtime ~ year, analysis\n\n\n\nhypothesis testing\n\nbootstrap\n\nregression\n\n\n\n\n\n\nJul 21, 2023\n\n\nPrateek\n\n7 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/index.html#modelling",
    "href": "posts/occupation_impact_on_rentals/index.html#modelling",
    "title": "rentership ~ occupation, analysis",
    "section": "Modelling",
    "text": "Modelling\n\nBaseline Model: Fixed effects without location information\nWe refer to data at MSOA level for modelling. For the baseline model, MSOA variable is excluded and Elementary occupations are treated as a reference category for Occupation variable. aged 15 years and under variable is dropped as it is irrelevant for this study. It is immediately obvious that occupation has significant association with proportion of people renting. \n\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(renting_total, occupation_total - renting_total)\n\nTerms added sequentially (first to last)\n\n            Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                        18494     338719              \noccupation   8    59415     18486     279305 &lt; 2.2e-16 ***\nage_25_34    1    74323     18485     204982 &lt; 2.2e-16 ***\nage_35_49    1     1804     18484     203178 &lt; 2.2e-16 ***\nage_50_64    1    28045     18483     175133 &lt; 2.2e-16 ***\nage_over_65  1      171     18482     174961 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe diagnostics look assuring and at this point, outliers do not appear to be significantly harmful.\n\n\n\n\n\n\n\n\n\nIt can be investigated to see which data points are causing this.  Manual inspection can be aided by geographic location data MSOA Map\n\ne02002991 : Is in the middle of Bath town Center and has much higher proportion of Professionals than what model predicts.\ne02006841 : Is a high leverage point. Is in the center of Gateshead, which has slightly higher proportion of Professionals than what model predicts. High leverage comes from 61% Professionals falling in age_25_34 bracket. But the residuals indicate that the influence of this data point is not high\ne02007095: Dinnington, has a very low proportion of rentership (6%), which causes a high value of residuals. This is an affluent, semi-rural village with many of the houses been bought under right to buy legislation (“Newcastle Residential Areas,” n.d.).\ne02005775: Harrogate, rentership is severly underpredicted due to 27% Professionals falling in 50-64 age bracket, leading to a high residual.\n\n\n\n\n\n\nage_16_24\nage_25_34\nage_35_49\nage_50_64\nage_over_65\n\n\n\n\n0.0971357\n0.3430884\n0.2826899\n0.2353674\n0.0417186\n\n\n0.1165957\n0.6153191\n0.2017021\n0.0578723\n0.0085106\n\n\n0.0388693\n0.3951708\n0.4517079\n0.1060071\n0.0082450\n\n\n0.0752961\n0.2876481\n0.3307953\n0.2724196\n0.0338409\n\n\n\n\n\n\n\nUsing location information in the model\nEach MSOA can have its own level of rentership that needs to be accounted for (and town centers can be very different from suburbs).  This is an opportunity for us to introduce location information (LAD and MSOA) in the model. We shall be mindful of the fact that the MSOA and LAD are randomly chosen and that MSOA are nested inside LAD.\nDiagnostics have improved and residuals are smaller than before, so the addition of MSOAs in model is worthwhile. This is also confirmed in the ANOVA output.\n\n\n\n\n\n\n\n\n\nData: dataset\nModels:\nglmod_base: cbind(renting_total, occupation_total - renting_total) ~ occupation + age_25_34 + age_35_49 + age_50_64 + age_over_65\nglmermod: cbind(renting_total, occupation_total - renting_total) ~ occupation + age_25_34 + age_35_49 + age_50_64 + age_over_65 + (1 | lad_name) + (1 | lad_name:msoa)\n           npar    AIC    BIC  logLik -2*log(L)  Chisq Df Pr(&gt;Chisq)    \nglmod_base   13 272503 272605 -136238    272477                         \nglmermod     15 147626 147744  -73798    147596 124881  2  &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nRandom effects of LAD appear reasonably normally distributed.  MSOA nested inside LAD are slighly more dispersed and have longer tails. The worst residual point lies in Catterick Garrison, which is a small town with a large army base and unusually high rentership of Associate professionals in that area.\n\n\n\n\n\n\n\n\n\nBefore moving to inference using this model, it is important to check how it fits. ChiSq GoF test suggests p-value 0, which indicates that there is still quite a lot of variance in data that isn’t explained by the current model. \nThe dispersion parameter is 2.15. This indicates that dispersion is not too extreme and we chose not to address this.\nLooking at the coefficients of Occupation types, since Elementary Occupations was reference class, the odds for any Occupation are relative to it.  In general any Occupation category is less likely to rent compared to Elementary Occupations, in particular Professionals are nearly 40% less likely (0.6x) to rent.\n\n\n\n\n\n\n\n\n\nAge plays a key role in likelihood of renting too. Highest rentership can be observed in 25-34 year olds. This is almost 4x more (300% more) compared to 16-24 year olds.  This is likely because young adults aged 16-24 rarely act as HRPs for rented homes; instead, they are typically dependent residents in owner-occupied parental households."
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#data",
    "href": "posts/user_behaviour_analysis/index.html#data",
    "title": "Probabilistic model of User journey",
    "section": "Data",
    "text": "Data\nWe use a simulated dataset based on historical data of a SaaS app.  The data is available here and contains three columns: user_id, date, and registration_date.\nEach record indicates a day when a user was active. \n\n\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\nd8c465ab-e9fd-5edd-9e4e-c77094700cb5\n2020-10-01\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-01\n2020-08-25\n\n\nbfeac474-5b66-566f-8654-262bb79c873e\n2020-10-01\n2020-05-31\n\n\nd32fcac5-122c-5463-8aea-01b39b9ad0bb\n2020-10-01\n2020-09-30\n\n\nc1ece677-e643-5bb3-8701-f1c59a0bf4cd\n2020-10-01\n2020-09-05\n\n\nba6750c9-9b60-58dc-8d40-504e7375249e\n2020-10-01\n2020-09-29\n\n\n\n\n\n\n\nTotal users: 51480\n\n\nDate range: 2020-10-01 to 2023-10-31\n\n\nThis is how DAU timeseries looks"
  },
  {
    "objectID": "posts/user_behaviour_analysis/index.html#model",
    "href": "posts/user_behaviour_analysis/index.html#model",
    "title": "Probabilistic model of User journey",
    "section": "Model",
    "text": "Model\n\nFuture new user count model\nTo even begin thinking about transitions on a future date, we first need to acknowledge the fact that new user counts will be a significant missing piece of information in the analysis.  To get around this limitation, we shall first build a new user count prediction model that need not be very accurate (since in a realistic setting new user counts can also be a parameter that can be changed).\nThe information on new users on any given date is inherent in registration_date in the data.\nAfter some experimentation, the model setup to predict future counts of new users is as above.  The counts are assumed Poisson but with a higher variance than normal due to spikes in the data and hence a negative binomial model has been used.  It was imperative that we used a robust version of GLM to be able to get relatively constant variance of errors, that were being caused due to outliers in user count towards beginning of every year.  The model appears to be doing a satisfactory job of predicting counts of new users and we consider this acceptable for further analysis.\n\n\n\n\n\n\n\n\n\n\n\nAssigning states\nThere needs to be an assignment of one of the 7 states (mentioned earlier) against each date for a user. This will help identify transitions made between states and populate the Transition matrix. However, the data only consists of records of active days, we need to explicitly extend them and include the days when a user was not active. In other words, instead of this table of records:\n\n\n\nuser_id\ndate\nregistration_date\n\n\n\n\n1234567\n2023-01-01\n2023-01-01\n\n\n1234567\n2023-01-03\n2023-01-01\n\n\n\nwe’d like to get a table like this:\n\n\n\nuser_id\ndate\nis_active\nregistration_date\n\n\n\n\n1234567\n2023-01-01\nTRUE\n2023-01-01\n\n\n1234567\n2023-01-02\nFALSE\n2023-01-01\n\n\n1234567\n2023-01-03\nTRUE\n2023-01-01\n\n\n1234567\n2023-01-04\nFALSE\n2023-01-01\n\n\n…\n…\n…\n…\n\n\n1234567\n2023-10-31\nFALSE\n2023-01-01\n\n\n\nAnd using this table, subsequently estimate states using simple rules (mentioned in definitions above) for each user_id, corresponding to each day in the data.\nThe volume of data at our disposal makes this a challenging task to run conveniently on a small machine, so the state determination is performed offline and the code to do it can be found here\n\n\n\n\n\n\n\n\n\n\n\n\nuser_id\ndate\nis_active\nstate\nregistration_date\n\n\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-18\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-19\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-20\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-21\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-22\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-23\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-24\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-25\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-26\nFALSE\nat_risk_wau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-27\nFALSE\nat_risk_mau\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-28\nTRUE\nreactivated\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-29\nTRUE\ncurrent\n2020-08-25\n\n\n269b7f13-a509-5174-85cb-95a8f7b932e8\n2020-10-30\nTRUE\ncurrent\n2020-08-25\n\n\n\n\n\n\n\nCalculating transition matrix\nWe need a flexible approach to calculate transition matrix over any arbitrary period of time. This is because it is likely that transition matrix changes over time as user preferences and product itself evolves. \n\nget_transition_matrix &lt;- function(states_slice) {\n        M &lt;- states_slice %&gt;%\n            group_by(user_id) %&gt;%\n            arrange(date) %&gt;%\n            mutate(\n                state_to = lead(state),\n                state_from = state\n            ) %&gt;%\n            ungroup() %&gt;%\n            # Remove the last row since user doesn't transition to anywhere from that\n            filter(!is.na(state_to)) %&gt;%\n            # Count transitions per user to avoid counting transitions across users\n            group_by(user_id, state_from, state_to) %&gt;%\n            summarise(transition_count = n()) %&gt;%\n            ungroup() %&gt;%\n            # Sum all the transitions over all users\n            group_by(state_from, state_to) %&gt;%\n            summarise(transition_count = sum(transition_count)) %&gt;%\n            ungroup() %&gt;%\n            # Generates a wide form matrix from long form\n            pivot_wider(names_from = state_to, values_from = transition_count, values_fill = list(transition_count = 0)) %&gt;%\n            # Normalizes matrix of counts so that rows sum to 1\n            mutate(row_sum = rowSums(dplyr::select(., -state_from))) %&gt;%\n            mutate(across(-state_from, ~ . / row_sum)) %&gt;%\n            dplyr::select(-row_sum) %&gt;%\n            # Generates ordered matrix where row and column name orders match\n            mutate(new = 0) %&gt;% # To get consistent matrix since 'new' is missing from state_to\n            column_to_rownames(\"state_from\") %&gt;%\n            .[order(rownames(.)), order(colnames(.))]\n    return(M)\n}\nEssentially we seek to calculate a matrix which has state_from in its rows state_to as its column and fraction of transitions as values. All the values in a row must sum to 1 obeying the fact that from a given state a user will end up in one of the 7 states.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nat_risk_mau\nat_risk_wau\ncurrent\ndormant\nnew\nreactivated\nresurrected\n\n\n\n\nat_risk_mau\n0.9520158\n0.0000000\n0.0000000\n0.0384384\n0\n0.0093866\n0.0001591\n\n\nat_risk_wau\n0.1308719\n0.7663644\n0.0982922\n0.0000000\n0\n0.0044715\n0.0000000\n\n\ncurrent\n0.0000000\n0.1487694\n0.8512306\n0.0000000\n0\n0.0000000\n0.0000000\n\n\ndormant\n0.0000000\n0.0000000\n0.0000000\n0.9996200\n0\n0.0000000\n0.0003800\n\n\nnew\n0.0000000\n0.4836773\n0.5163227\n0.0000000\n0\n0.0000000\n0.0000000\n\n\nreactivated\n0.0000000\n0.6353907\n0.3646093\n0.0000000\n0\n0.0000000\n0.0000000\n\n\nresurrected\n0.0000000\n0.6848495\n0.3151505\n0.0000000\n0\n0.0000000\n0.0000000\n\n\n\n\n\nAs a sanity check, we can analyse the matrix itself.  It makes sense that current -&gt; current transition value is high. This means that active users tend to continue being active with high probability or otherwise become at_risk_wau.  It also seems sensible that there is no state from which one could transition to new. Apparantly quite a high proportion of dormant users stay dormant which could be one of the focuses of resurrection campaigns in such companies.\n\n\nPredicting DAU\nWe start with getting initial state counts for the date on which the dataset ends. This combined with M transition matrix should give us new state counts. We will be mindful to update new_user_count for all the days we would like to predict on. This is done using the prediction model developed earlier.\nWe shall define a flexible mechanism to predict DAU, to enable changing prediction horizon. We will also supply a dataframe of new_users to this mechanism so that it may use predicted values for appropriate prediction dates.\n\npredict_dau &lt;- function(M, state0, start_date, end_date, new_users_predicted) {\n    dates &lt;- seq(as.Date(start_date), as.Date(end_date), by = \"day\")\n    # Align state0 name order with transition Matrix name order\n    new_dau &lt;- state0[match(state0$state, rownames(M)), ]$cnt\n    dau_pred &lt;- list()\n\n    for (dt in dates) {\n        new_dau &lt;- as.integer(t(M) %*% new_dau) # Transitions\n        new_users_today &lt;- new_users_predicted %&gt;%\n            filter(date == dt) %&gt;%\n            pull(predicted) %&gt;%\n            as.integer()\n        new_dau[5] &lt;- new_users_today # Today's new users get transitioned next day\n        dau_pred &lt;- append(dau_pred, list(new_dau))\n    }\n\n    df &lt;- data.frame(do.call(rbind, dau_pred))\n    colnames(df) &lt;- rownames(M)\n    df$date &lt;- dates\n    df$dau &lt;- df$new + df$current + df$reactivated + df$resurrected\n    df$wau &lt;- df$dau + df$at_risk_wau\n    df$mau &lt;- df$dau + df$at_risk_wau + df$at_risk_mau\n\n    return(tibble(df))\n}\n\n\n\n\n\n\n\n\n\nThe prominent contributer to DAU counts are current users, who are aided by addition of new users everyday and the fact that current users stay active with a probability of 85% (from M matrix).  The prediction makes intuitive sense however the magnitude of spike certainly seems quite large.  Observing the shape of predicted DAU, it seems quite strongly influenced by predicted new user count from the model earlier. Additionally, some degree of experimentation with the predictions suggest that about 20% decrease in new user counts bring predicted DAU to comparable levels of previous years (dotted line in the plot)."
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/index.html#introduction",
    "href": "posts/occupation_impact_on_rentals/index.html#introduction",
    "title": "rentership ~ occupation, analysis",
    "section": "",
    "text": "Private renting has expanded across the UK, yet the propensity to rent varies widely across occupations and life stages. This study quantifies how occupation and age relate to private rentership using 2021 ONS Census cross‑tabulations at the MSOA (neighbourhood‑scale) level. Housing tenure for household reference persons is linked to occupational categories and augmented with local authority identifiers to capture geographic context.\nThe analysis addresses three questions: \n\nDo higher‑skilled occupations rent less than lower‑skilled groups after accounting for age composition?\nHow does rentership vary across life stages (age) ?\nTo what extent do local authorities and neighbourhoods moderate these relationships?\n\nThe study is descriptive and subject to measurement constraints: age composition is observed for all workers rather than HRPs, and the focus is on full‑time employment. Despite these limitations, the findings provide a transparent baseline on how occupational structure and local context shape rental demand and motivate future work with richer microdata and dynamic market indicators."
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/index.html#analysis",
    "href": "posts/occupation_impact_on_rentals/index.html#analysis",
    "title": "rentership ~ occupation, analysis",
    "section": "Analysis",
    "text": "Analysis\n\nBaseline Model: Fixed effects without location information\nWe refer to data at MSOA level for modelling. For the baseline model, MSOA variable is excluded and Elementary occupations are treated as a reference category for Occupation variable. aged 15 years and under variable is dropped as it is irrelevant for this study. It is immediately obvious that occupation has significant association with proportion of people renting. \n\n\nAnalysis of Deviance Table\n\nModel: binomial, link: logit\n\nResponse: cbind(renting_total, occupation_total - renting_total)\n\nTerms added sequentially (first to last)\n\n                                Df Deviance Resid. Df Resid. Dev  Pr(&gt;Chi)    \nNULL                                            20123     341677              \noccupation                       8    58729     20115     282948 &lt; 2.2e-16 ***\nlog(age_16_24 + 1)               1    20639     20114     262308 &lt; 2.2e-16 ***\nlog(age_25_34 + 1)               1    11520     20113     250788 &lt; 2.2e-16 ***\nlog(age_35_49 + 1)               1    28124     20112     222664 &lt; 2.2e-16 ***\nlog(age_50_64 + 1)               1    19545     20111     203119 &lt; 2.2e-16 ***\nlog(age_over_65 + 1)             1     3530     20110     199589 &lt; 2.2e-16 ***\noccupation:log(age_16_24 + 1)    8     6347     20102     193241 &lt; 2.2e-16 ***\noccupation:log(age_25_34 + 1)    8     1222     20094     192020 &lt; 2.2e-16 ***\noccupation:log(age_35_49 + 1)    8     1383     20086     190637 &lt; 2.2e-16 ***\noccupation:log(age_50_64 + 1)    8     3627     20078     187010 &lt; 2.2e-16 ***\noccupation:log(age_over_65 + 1)  8     2519     20070     184492 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe Scale-location diagnostics and QQ-Residuals suggest the dispersion is much higher than expected. This is also confirmed by the dispersion parameter, 9.33\n\n\n\n\n\n\n\n\n\nHigh residual points can be investigated to identify some issues.  Primarily, these seem to areas in the city centers where rentership is much higher than what the model predicts and can be seen on the map too. \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmsoa\nlad_name\noccupation\nage_16_24\nage_25_34\nage_35_49\nage_50_64\nage_over_65\nrenting_total\noccupation_total\n.resid\nactual_renting_proportion\npredicted_proportion\n\n\n\n\ne02001432\nsefton\n9. elementary occupations\n149\n195\n238\n197\n34\n382\n477\n18.31027\n0.80\n0.39\n\n\ne02003293\nsouthend-on-sea\n6. caring, leisure and other service occupations\n71\n158\n247\n197\n31\n307\n436\n17.71392\n0.70\n0.29\n\n\ne02003516\nbrighton and hove\n2. professional occupations\n97\n590\n531\n289\n36\n662\n1071\n17.47028\n0.62\n0.36\n\n\ne02003430\nwindsor and maidenhead\n2. professional occupations\n36\n208\n347\n290\n30\n242\n506\n17.17201\n0.48\n0.15\n\n\ne02004365\neastbourne\n6. caring, leisure and other service occupations\n108\n203\n287\n185\n21\n326\n442\n17.08044\n0.74\n0.34\n\n\ne02003514\nbrighton and hove\n3. associate professional and technical occupations\n95\n434\n389\n179\n19\n447\n728\n16.96523\n0.61\n0.31\n\n\ne02003293\nsouthend-on-sea\n5. skilled trades occupations\n55\n131\n214\n151\n16\n251\n386\n16.82279\n0.65\n0.24\n\n\ne02003226\nswindon\n2. professional occupations\n44\n240\n262\n73\n4\n276\n425\n16.41195\n0.65\n0.27\n\n\ne02004365\neastbourne\n5. skilled trades occupations\n48\n104\n155\n113\n7\n203\n297\n15.97221\n0.68\n0.24\n\n\ne02004378\nhastings\n3. associate professional and technical occupations\n24\n137\n238\n198\n36\n213\n455\n15.84288\n0.47\n0.15\n\n\n\n\n\n\n\n\n\n\n\n\nOverall it appears that including location information in the model is necessary to account for area-specific rentership patterns.\n\n\nIntroducing location context in the model\nEach MSOA can have its own level of rentership that needs to be accounted for (and town centers can be very different from suburbs).  This is an opportunity for us to introduce location information (LAD and MSOA) in the model. We shall be mindful of the fact that the MSOA are randomly chosen and that MSOA are nested inside LAD.\n\n\nModel evaluation\nDiagnostics have improved and residuals are smaller than before, so the addition of MSOAs in model is worthwhile. \n\n\n\n\n\n\n\n\n\n\nError by occupation\nThe model captures renting behaviour across different occupations reasonably well and does not seem to have biased results for any.  An avenue for improvement can be with more information relevant to Elementary occupations and Professional occupations where the model seems to make large errors compared to others.\n\n\n\n\n\n\n\n\n\nThe model is robust to how many people are engaged in particular occupations. It seems to make similar magnitudes of errors and on average is unbiased. For e.g. the accuracy is coparalble at a location fewer occupants are Managers vs a location where several occupants are Managers.\n\n\n\n\n\n\n\n\n\n\n\nComparison\nComparing predictions from the 2 models on the list of worse cases highlighted earlier, there again seems to be an improvement.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmsoa\nlad_name\noccupation\nrenting_total\noccupation_total\nactual_renting_proportion\npredicted_proportion_base\npredicted_proportion_glmer\n\n\n\n\ne02001432\nsefton\n9. elementary occupations\n382\n477\n0.80\n0.39\n0.72\n\n\ne02003293\nsouthend-on-sea\n6. caring, leisure and other service occupations\n307\n436\n0.70\n0.29\n0.63\n\n\ne02003516\nbrighton and hove\n2. professional occupations\n662\n1071\n0.62\n0.36\n0.61\n\n\ne02003430\nwindsor and maidenhead\n2. professional occupations\n242\n506\n0.48\n0.15\n0.35\n\n\ne02004365\neastbourne\n6. caring, leisure and other service occupations\n326\n442\n0.74\n0.34\n0.70\n\n\ne02003514\nbrighton and hove\n3. associate professional and technical occupations\n447\n728\n0.61\n0.31\n0.59\n\n\ne02003293\nsouthend-on-sea\n5. skilled trades occupations\n251\n386\n0.65\n0.24\n0.58\n\n\ne02003226\nswindon\n2. professional occupations\n276\n425\n0.65\n0.27\n0.53\n\n\ne02004365\neastbourne\n5. skilled trades occupations\n203\n297\n0.68\n0.24\n0.63\n\n\ne02004378\nhastings\n3. associate professional and technical occupations\n213\n455\n0.47\n0.15\n0.49\n\n\n\n\n\n\n\nError behaviour spatially\nThe error in predicted probability of rentership is small for several MSOAs, even though a small subset was used in modelling. This is another good reason to retain the current model.\n\n\n\n\n\n\n\n\n\n\n\nRandom effects and dispersion\nRandom effects of LAD appear reasonably normally distributed.  MSOA nested inside LAD are slighly more dispersed and have longer tails, but it doesn’t seem too problematic to affect reliability of the model.\n\n\n\n\n\n\n\n\n\nBefore moving to inference using this model, it is important to check how it fits. ChiSq GoF test suggests p-value 0, which indicates that there is still quite a lot of variance in data that isn’t explained by the current model. \nThe dispersion parameter is 2.08. This indicates that there is some degree of overdispersion. It is likely a result of not including more factors affecting rentership (e.g. presence of employers, type of location, commute etc.) and using a simplistic model. The overdispersion is largely relevant for standard errors, but fixed effects which are our quantity of interest should be unchanged. Inference will likely be worse but with such a simplistic model we do not expect significant impact. We shall retain the current model as it is since it provides useful insights as we shall see in the following sections.\n\n\n\nInterpretation and application\n\nOccupation effect\nLooking at the coefficients of Occupation types, since Elementary Occupations was reference class, the odds for any Occupation are relative to it.  In general any Occupation category is less likely to rent compared to Elementary Occupations, in particular Professionals are nearly 55% less likely (0.44x) to rent.\n\n\n\n\n\n\n\n\n\n\n\nAge effect\nAge plays a key role in likelihood of renting too. Rentership is highly sensitive to population in 25-34 year age bracket. With a 10% increase in population in that age bracket, odds increase by approx 3.8% \n\n\n\n\n\n\n\n\n\n\n\nCompound effect on a sample of MSOA\nSupposing that a real estate investment firm wants to introduce new rental supply or wants to sell its existing portfolio in a location where the target consumer is employed as a Professional or in Sales and service, the elasticities associated with these changes can be estimated (as an example +/- 50 units change is demonstrated below).  This exercise is helpful with a test for the fact that new supply does not significantly change the underlying renting propensity (.pred). This would mean that under existing conditions, new supply can likely be absorbed by the market.  It is important to note that this evaluation is only from supply point of view. It is possible that price elasticity of supply can limit absorption much earlier than what is proposed here. Neverthless, the idea here tests for stability in the market and that may implicitly imply stability of some phenomenon that affect price elasticity."
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/data_preparation.html",
    "href": "posts/occupation_impact_on_rentals/data_preparation.html",
    "title": "Data preparation",
    "section": "",
    "text": "- The project is out-of-sync -- use `renv::status()` for details."
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/data_preparation.html#msoa-level-tenure-occupation-data",
    "href": "posts/occupation_impact_on_rentals/data_preparation.html#msoa-level-tenure-occupation-data",
    "title": "Data preparation",
    "section": "MSOA level tenure-occupation data",
    "text": "MSOA level tenure-occupation data"
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/data_preparation.html#additional-data",
    "href": "posts/occupation_impact_on_rentals/data_preparation.html#additional-data",
    "title": "Data preparation",
    "section": "Additional data",
    "text": "Additional data\n\nGeo data\n\n\nMSOA level occupation-age data"
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/data_preparation.html#dataset",
    "href": "posts/occupation_impact_on_rentals/data_preparation.html#dataset",
    "title": "Data preparation",
    "section": "Dataset",
    "text": "Dataset\n\nFiltered\n\n\nSubsampled"
  },
  {
    "objectID": "posts/occupation_impact_on_rentals/index.html#conclusion",
    "href": "posts/occupation_impact_on_rentals/index.html#conclusion",
    "title": "rentership ~ occupation, analysis",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis provides descriptive evidence that who rents, and where, is shaped by occupation and age. Across UK MSOAs (excluding London), higher‑skilled groups—especially professionals—are consistently less likely to rent than lower‑skilled groups. Rentership peaks among 25–34 year olds, and remains sensitive to the age mix in local working populations. Introducing local authority and neighbourhood random effects improves fit and reveals meaningful spatial variation: town centres and dense urban MSOAs tend to have higher rentership than suburban areas, even after adjusting for occupation and age.\nThe models are simple and intentionally transparent. While they capture core patterns, residual variance and mild overdispersion indicate omitted local factors (e.g., price levels, employer presence, transport access) still matter. Results should be interpreted as associations, not causal effects.\nSeveral limitations are important. Age‑by‑occupation counts refer to all workers rather than household reference persons, creating a measurement disconnect with tenure data. London is excluded, so findings do not generalise to its market. The analysis is cross‑sectional and does not track transitions into or out of renting.\nDespite these caveats, the evidence suggests rental demand is highest where younger workers are concentrated and falls with occupational skill. For planning and housing supply, this points to tailoring new rental provision toward places with strong 25–34 cohorts and occupations more likely to rent."
  }
]